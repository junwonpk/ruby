{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Ruby - Baseline\n",
    "\n",
    "## Goals: \n",
    "1. Predict how many comments will follow a given comment.\n",
    "2. Generate comments that will generate most children comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Comment:\n",
      "{'body': \"'Cause your dick might hit a butt and make baby Jesus cry. \", 'author': 'Number6isNo1', 'subreddit': 'news', 'id': 'dbumnrk', 'parent_id': 't1_dbumi82', 'score': 26, 'controversiality': 0, 'link_id': 't3_5lbkgm'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def read_dataset():\n",
    "    comments = []\n",
    "    features = ['body', 'author', 'subreddit', 'id', 'parent_id', 'score', 'controversiality', 'link_id']\n",
    "    subreddits = ['news']\n",
    "    comments_size = 100000\n",
    "    years = [\"2017\"]\n",
    "    months = [\"01\"]\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            filename = \"reddit/\" + year + \"/RC_\" + year + \"-\" + month\n",
    "            with open(filename, \"r\") as data:\n",
    "                for line in data:\n",
    "                    if len(comments) < comments_size:\n",
    "                        comment_data = json.loads(line)\n",
    "                        if comment_data['subreddit'] in subreddits:\n",
    "                            comment = dict()\n",
    "                            for feature in features:\n",
    "                                comment[feature] = comment_data[feature]\n",
    "                            comments.append(comment)\n",
    "                    else:\n",
    "                        break\n",
    "    print (\"Example Comment:\")\n",
    "    print (comments[0])\n",
    "    return comments\n",
    "\n",
    "comments = read_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline 1: Logistic Regression with Unigram Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def build_vocabulary(comments):\n",
    "    vocabulary = set()\n",
    "    for comment in comments:\n",
    "        for word in comment:\n",
    "            vocabulary.add(word)\n",
    "    return vocabulary\n",
    "\n",
    "def prepare_train(comments, vocabulary, train_size):\n",
    "    train = comments[:int(train_size * len(comments))]\n",
    "    test = comments[len(comments)-len(train):]\n",
    "    vocabulary = list(vocabulary)\n",
    "    cv = CountVectorizer(vocabulary=vocabulary)\n",
    "    \n",
    "    train_X = []\n",
    "    train_Y = []\n",
    "    test_X = []\n",
    "    test_Y = []\n",
    "\n",
    "    for comment in train:\n",
    "        train_X.append(comment['body'])\n",
    "        train_Y.append(comment['score'])\n",
    "    train_X = cv.fit_transform(train_X)\n",
    "\n",
    "    for comment in test:\n",
    "        test_X.append(comment['body'])\n",
    "        test_Y.append(comment['score'])\n",
    "    test_X = cv.fit_transform(test_X)\n",
    "    \n",
    "    return train_X, train_Y, test_X, test_Y\n",
    "\n",
    "vocabulary = build_vocabulary(comments)\n",
    "train_X, train_Y, test_X, test_Y = prepare_train(comments, vocabulary, train_size=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.3401532695e-06\n",
      "21.3334794347\n",
      "34704.3763216\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr = lr.fit(train_X, train_Y)\n",
    "print (lr.score(train_X, train_Y))\n",
    "prediction = lr.predict(test_X)\n",
    "# prediction = np.zeros(len(test_Y))\n",
    "print (metrics.mean_absolute_error(test_Y, prediction))\n",
    "print (metrics.mean_squared_error(test_Y, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Baseline 2: Neural Network Regression with GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.775162398815155)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = 'glove.txt'\n",
    "word2vec_output_file = 'word2vec.txt'\n",
    "model = glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "filename = 'word2vec.txt'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def prepare_train(comments, train_size):\n",
    "    train = comments[:int(train_size * len(comments))]\n",
    "    test = comments[len(comments)-len(train):]\n",
    "    wordvec_dim = model['test'].shape\n",
    "    \n",
    "    train_X = []\n",
    "    train_Y = []\n",
    "    test_X = []\n",
    "    test_Y = []\n",
    "\n",
    "    for comment in train:\n",
    "        sentence_vector = np.zeros(wordvec_dim)\n",
    "        count = 0\n",
    "        for word in comment['body']:\n",
    "            if word in model.vocab:\n",
    "                count += 1\n",
    "                sentence_vector += model[word]\n",
    "        sentence_vector /= count\n",
    "        train_X.append(sentence_vector)\n",
    "        train_Y.append(comment['score'])\n",
    "\n",
    "    for comment in test:\n",
    "        sentence_vector = np.zeros(wordvec_dim)\n",
    "        count = 0\n",
    "        for word in comment['body']:\n",
    "            if word in model.vocab:\n",
    "                count += 1\n",
    "                sentence_vector += model[word]\n",
    "        sentence_vector /= count\n",
    "        test_X.append(sentence_vector)\n",
    "        test_Y.append(comment['score'])\n",
    "    \n",
    "    return train_X, train_Y, test_X, test_Y\n",
    "\n",
    "train_X, train_Y, test_X, test_Y = prepare_train(comments, train_size=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0713716073349\n",
      "0.219620100935\n",
      "0.233813820107\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nn = MLPRegressor(\n",
    "    hidden_layer_sizes=(100,) * 4,  activation='relu', solver='adam', alpha=0.001, batch_size='auto',\n",
    "    learning_rate='constant', learning_rate_init=0.01, power_t=0.5, max_iter=1000, shuffle=True,\n",
    "    random_state=9, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True,\n",
    "    early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "nn.fit(train_X, train_Y)\n",
    "prediction = nn.predict(test_X)\n",
    "print (nn.score(train_X, train_Y))\n",
    "print (metrics.mean_absolute_error(test_Y, prediction))\n",
    "print (metrics.mean_squared_error(test_Y, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Baseline 3: Predicting Number of Child Comments\n",
    "with Neural Network and GloVe Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.775162398815155)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = 'glove.txt'\n",
    "word2vec_output_file = 'word2vec.txt'\n",
    "model = glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "filename = 'word2vec.txt'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Comment:\n",
      "{'body': 'I think it was more for the fact that they were saying to do it, in order to save money.', 'author': 'potrg801', 'subreddit': 'news', 'id': 'dbumnt2', 'parent_id': 't1_dbtt6ak', 'score': 1, 'controversiality': 0, 'link_id': 't3_5l77ad', 'num_child_comments': 0}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def read_dataset():\n",
    "    comments = []\n",
    "    features = ['body', 'author', 'subreddit', 'id', 'parent_id', 'score', 'controversiality', 'link_id', \"num_child_comments\"]\n",
    "    subreddits = ['news']\n",
    "    comments_size = 100000\n",
    "    years = [\"2017\"]\n",
    "    months = [\"01\"]\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            filename = \"reddit/\" + year + \"/RC_\" + year + \"-\" + month + \"A\"\n",
    "            with open(filename, \"r\") as data:\n",
    "                for line in data:\n",
    "                    if len(comments) < comments_size:\n",
    "                        comment_data = json.loads(line)\n",
    "                        if comment_data['subreddit'] in subreddits:\n",
    "                            comment = dict()\n",
    "                            for feature in features:\n",
    "                                comment[feature] = comment_data[feature]\n",
    "                            comments.append(comment)\n",
    "                    else:\n",
    "                        break\n",
    "    print (\"Example Comment:\")\n",
    "    print (comments[1])\n",
    "    return comments\n",
    "\n",
    "comments = read_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def prepare_train(comments, train_size):\n",
    "    train = comments[:int(train_size * len(comments))]\n",
    "    test = comments[len(comments)-len(train):]\n",
    "    wordvec_dim = model['test'].shape\n",
    "    \n",
    "    train_X = []\n",
    "    train_Y = []\n",
    "    test_X = []\n",
    "    test_Y = []\n",
    "\n",
    "    for comment in train:\n",
    "        sentence_vector = np.zeros(wordvec_dim)\n",
    "        count = 0\n",
    "        for word in comment['body']:\n",
    "            if word in model.vocab:\n",
    "                count += 1\n",
    "                sentence_vector += model[word]\n",
    "#         sentence_vector /= count\n",
    "        train_X.append(sentence_vector)\n",
    "        train_Y.append(comment['num_child_comments'])\n",
    "\n",
    "    for comment in test:\n",
    "        sentence_vector = np.zeros(wordvec_dim)\n",
    "        count = 0\n",
    "        for word in comment['body']:\n",
    "            if word in model.vocab:\n",
    "                count += 1\n",
    "                sentence_vector += model[word]\n",
    "#         sentence_vector /= count\n",
    "        test_X.append(sentence_vector)\n",
    "        test_Y.append(comment['num_child_comments'])\n",
    "    \n",
    "    return train_X, train_Y, test_X, test_Y\n",
    "\n",
    "train_X, train_Y, test_X, test_Y = prepare_train(comments, train_size=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.34002538221\n",
      "0.284969198496\n",
      "1.11329125027\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nn = MLPRegressor(\n",
    "    hidden_layer_sizes=(100,) * 4,  activation='relu', solver='adam', alpha=0.001, batch_size='auto',\n",
    "    learning_rate='constant', learning_rate_init=0.01, power_t=0.5, max_iter=1000, shuffle=True,\n",
    "    random_state=9, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True,\n",
    "    early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "nn.fit(train_X, train_Y)\n",
    "prediction = nn.predict(test_X)\n",
    "# prediction = np.zeros(len(test_X))\n",
    "print (nn.score(train_X, train_Y))\n",
    "print (metrics.mean_absolute_error(test_Y, prediction))\n",
    "print (metrics.mean_squared_error(test_Y, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline 5: Predicting Number of Child Comments\n",
    "with Neural Network and GloVe Vectors + Exclude comments with zero child comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Comment:\n",
      "{'body': '[removed]', 'author': '[deleted]', 'subreddit': 'news', 'id': 'dbumpip', 'parent_id': 't3_5l5aew', 'score': 1, 'controversiality': 0, 'link_id': 't3_5l5aew', 'num_child_comments': 1}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def read_dataset():\n",
    "    comments = []\n",
    "    features = ['body', 'author', 'subreddit', 'id', 'parent_id', 'score', 'controversiality', 'link_id', \"num_child_comments\"]\n",
    "    subreddits = ['news']\n",
    "    comments_size = 10000\n",
    "    years = [\"2017\"]\n",
    "    months = [\"01\"]\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            filename = \"reddit/\" + year + \"/RC_\" + year + \"-\" + month + \"A\"\n",
    "            with open(filename, \"r\") as data:\n",
    "                for line in data:\n",
    "                    if len(comments) < comments_size:\n",
    "                        comment_data = json.loads(line)\n",
    "                        if comment_data['subreddit'] in subreddits and comment_data['num_child_comments'] > 0:\n",
    "                            comment = dict()\n",
    "                            for feature in features:\n",
    "                                comment[feature] = comment_data[feature]\n",
    "                            comments.append(comment)\n",
    "                    else:\n",
    "                        break\n",
    "    print (\"Example Comment:\")\n",
    "    print (comments[1])\n",
    "    return comments\n",
    "\n",
    "comments = read_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def prepare_train(comments, train_size):\n",
    "    train = comments[:int(train_size * len(comments))]\n",
    "    test = comments[len(comments)-len(train):]\n",
    "    wordvec_dim = model['test'].shape\n",
    "    \n",
    "    train_X = []\n",
    "    train_Y = []\n",
    "    test_X = []\n",
    "    test_Y = []\n",
    "\n",
    "    for comment in train:\n",
    "        sentence_vector = np.zeros(wordvec_dim)\n",
    "        count = 0\n",
    "        for word in comment['body']:\n",
    "            if word in model.vocab:\n",
    "                count += 1\n",
    "                sentence_vector += model[word]\n",
    "#         sentence_vector /= count\n",
    "        train_X.append(sentence_vector)\n",
    "        train_Y.append(comment['num_child_comments'])\n",
    "\n",
    "    for comment in test:\n",
    "        sentence_vector = np.zeros(wordvec_dim)\n",
    "        count = 0\n",
    "        for word in comment['body']:\n",
    "            if word in model.vocab:\n",
    "                count += 1\n",
    "                sentence_vector += model[word]\n",
    "#         sentence_vector /= count\n",
    "        test_X.append(sentence_vector)\n",
    "        test_Y.append(comment['num_child_comments'])\n",
    "    \n",
    "    return train_X, train_Y, test_X, test_Y\n",
    "\n",
    "train_X, train_Y, test_X, test_Y = prepare_train(comments, train_size=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.00064974642348\n",
      "7.65527630524\n",
      "2218.28397051\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nn = MLPRegressor(\n",
    "    hidden_layer_sizes=(100,) * 4,  activation='relu', solver='adam', alpha=0.001, batch_size='auto',\n",
    "    learning_rate='constant', learning_rate_init=0.01, power_t=0.5, max_iter=1000, shuffle=True,\n",
    "    random_state=9, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True,\n",
    "    early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "nn.fit(train_X, train_Y)\n",
    "prediction = nn.predict(test_X)\n",
    "print (nn.score(train_X, train_Y))\n",
    "print (metrics.mean_absolute_error(test_Y, prediction))\n",
    "print (metrics.mean_squared_error(test_Y, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Baseline 6: Neural Network with GloVe (Larger Dataset)\n",
    "On 2016 Reddit Comments from r/news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bucketizing into 5 buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "filename = 'word2vec.txt'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "print(model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1))\n",
    "\n",
    "def load_comments(filename):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(filename, \"r\") as data:\n",
    "        for line in data:\n",
    "            comment = json.loads(line)\n",
    "            sentence_vector = np.zeros(model['test'].shape)\n",
    "            count = 0\n",
    "            for word in comment['body']:\n",
    "                if word in model.vocab:\n",
    "                    count += 1\n",
    "                    sentence_vector += model[word]\n",
    "#             sentence_vector /= count\n",
    "            X.append(sentence_vector)\n",
    "            bucket = 0\n",
    "            if comment['num_child_comments'] == 0:\n",
    "                bucket = 1\n",
    "            elif comment['num_child_comments'] >= 1 and comment['num_child_comments'] <= 2:\n",
    "                bucket = 2\n",
    "            elif comment['num_child_comments'] >= 3 and comment['num_child_comments'] <= 6:\n",
    "                bucket = 3\n",
    "            elif comment['num_child_comments'] >= 7 and comment['num_child_comments'] <= 14:\n",
    "                bucket = 4\n",
    "            elif comment['num_child_comments'] >= 15:\n",
    "                bucket = 5\n",
    "            Y.append(bucket)\n",
    "    return X, Y\n",
    "\n",
    "train_X, train_Y = load_comments('redditnews/RedditTrain')\n",
    "dev_X, dev_Y = load_comments('redditnews/RedditDev')\n",
    "print('Loaded Reddit Comments from 2016')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nn = MLPRegressor(\n",
    "    hidden_layer_sizes=(100,) * 4,  activation='relu', solver='adam', alpha=0.001, batch_size='auto',\n",
    "    learning_rate='constant', learning_rate_init=0.01, power_t=0.5, max_iter=1000, shuffle=True,\n",
    "    random_state=9, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True,\n",
    "    early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "nn.fit(train_X, train_Y)\n",
    "prediction = nn.predict(dev_X)\n",
    "print (nn.score(train_X, train_Y))\n",
    "print (metrics.mean_absolute_error(dev_Y, prediction))\n",
    "print (metrics.mean_squared_error(dev_Y, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nn = MLPClassifier(solver='adam', alpha=1e-5,\n",
    "                   hidden_layer_sizes=(100,) * 4, random_state=1)\n",
    "nn.fit(train_X, train_Y)\n",
    "prediction = nn.predict(dev_X)\n",
    "print (nn.score(train_X, train_Y))\n",
    "print (metrics.mean_absolute_error(dev_Y, prediction))\n",
    "print (metrics.mean_squared_error(dev_Y, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary (0 or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "filename = 'word2vec.txt'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "print(model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1))\n",
    "\n",
    "def load_comments(filename):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(filename, \"r\") as data:\n",
    "        for line in data:\n",
    "            comment = json.loads(line)\n",
    "            sentence_vector = np.zeros(model['test'].shape)\n",
    "            count = 0\n",
    "            for word in comment['body']:\n",
    "                if word in model.vocab:\n",
    "                    count += 1\n",
    "                    sentence_vector += model[word]\n",
    "#             sentence_vector /= count\n",
    "            X.append(sentence_vector)\n",
    "            bucket = 0\n",
    "            if comment['num_child_comments'] != 0:\n",
    "                bucket = 1\n",
    "            Y.append(bucket)\n",
    "    return X, Y\n",
    "\n",
    "train_X, train_Y = load_comments('redditnews/RedditTrain')\n",
    "dev_X, dev_Y = load_comments('redditnews/RedditDev')\n",
    "print('Loaded Reddit Comments from 2016')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nn = MLPClassifier(solver='adam', alpha=1e-5,\n",
    "                   hidden_layer_sizes=(100,) * 4, random_state=1)\n",
    "nn.fit(train_X, train_Y)\n",
    "prediction = nn.predict(dev_X)\n",
    "print (nn.score(train_X, train_Y))\n",
    "print (metrics.mean_absolute_error(dev_Y, prediction))\n",
    "print (metrics.mean_squared_error(dev_Y, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
