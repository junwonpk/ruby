{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation\n",
    "Reddit ID, PW = stanfordruby[1-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/junwonpk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import nltk\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from random import randrange\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelNetwork:\n",
    "    def __init__(self, in_size, lstm_size, num_layers, out_size, session,\n",
    "                 learning_rate=0.003, name=\"rnn\"):\n",
    "        self.scope = name\n",
    "        self.in_size = in_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.out_size = out_size\n",
    "        self.session = session\n",
    "        self.learning_rate = tf.constant(learning_rate)\n",
    "        # Last state of LSTM, used when running the network in TEST mode\n",
    "        self.lstm_last_state = np.zeros(\n",
    "            (self.num_layers * 2 * self.lstm_size,)\n",
    "        )\n",
    "        with tf.variable_scope(self.scope):\n",
    "            # (batch_size, timesteps, in_size)\n",
    "            self.xinput = tf.placeholder(\n",
    "                tf.float32,\n",
    "                shape=(None, None, self.in_size),\n",
    "                name=\"xinput\"\n",
    "            )\n",
    "            self.lstm_init_value = tf.placeholder(\n",
    "                tf.float32,\n",
    "                shape=(None, self.num_layers * 2 * self.lstm_size),\n",
    "                name=\"lstm_init_value\"\n",
    "            )\n",
    "            # LSTM\n",
    "            self.lstm_cells = [\n",
    "                tf.contrib.rnn.BasicLSTMCell(\n",
    "                    self.lstm_size,\n",
    "                    forget_bias=1.0,\n",
    "                    state_is_tuple=False\n",
    "                ) for i in range(self.num_layers)\n",
    "            ]\n",
    "            self.lstm = tf.contrib.rnn.MultiRNNCell(\n",
    "                self.lstm_cells,\n",
    "                state_is_tuple=False\n",
    "            )\n",
    "            # Iteratively compute output of recurrent network\n",
    "            outputs, self.lstm_new_state = tf.nn.dynamic_rnn(\n",
    "                self.lstm,\n",
    "                self.xinput,\n",
    "                initial_state=self.lstm_init_value,\n",
    "                dtype=tf.float32\n",
    "            )\n",
    "            # Linear activation (FC layer on top of the LSTM net)\n",
    "            self.rnn_out_W = tf.Variable(\n",
    "                tf.random_normal(\n",
    "                    (self.lstm_size, self.out_size),\n",
    "                    stddev=0.01\n",
    "                )\n",
    "            )\n",
    "            self.rnn_out_B = tf.Variable(\n",
    "                tf.random_normal(\n",
    "                    (self.out_size,), stddev=0.01\n",
    "                )\n",
    "            )\n",
    "            outputs_reshaped = tf.reshape(outputs, [-1, self.lstm_size])\n",
    "            network_output = tf.matmul(\n",
    "                outputs_reshaped,\n",
    "                self.rnn_out_W\n",
    "            ) + self.rnn_out_B\n",
    "            batch_time_shape = tf.shape(outputs)\n",
    "            self.final_outputs = tf.reshape(\n",
    "                tf.nn.softmax(network_output),\n",
    "                (batch_time_shape[0], batch_time_shape[1], self.out_size)\n",
    "            )\n",
    "            # Training: provide target outputs for supervised training.\n",
    "            self.y_batch = tf.placeholder(\n",
    "                tf.float32,\n",
    "                (None, None, self.out_size)\n",
    "            )\n",
    "            y_batch_long = tf.reshape(self.y_batch, [-1, self.out_size])\n",
    "            self.cost = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    logits=network_output,\n",
    "                    labels=y_batch_long\n",
    "                )\n",
    "            )\n",
    "            self.train_op = tf.train.RMSPropOptimizer(\n",
    "                self.learning_rate,\n",
    "                0.9\n",
    "            ).minimize(self.cost)\n",
    "\n",
    "    # Input: X is a single element, not a list!\n",
    "    def run_step(self, x, init_zero_state=True):\n",
    "        # Reset the initial state of the network.\n",
    "        if init_zero_state:\n",
    "            init_value = np.zeros((self.num_layers * 2 * self.lstm_size,))\n",
    "        else:\n",
    "            init_value = self.lstm_last_state\n",
    "        out, next_lstm_state = self.session.run(\n",
    "            [self.final_outputs, self.lstm_new_state],\n",
    "            feed_dict={\n",
    "                self.xinput: [x],\n",
    "                self.lstm_init_value: [init_value]\n",
    "            }\n",
    "        )\n",
    "        self.lstm_last_state = next_lstm_state[0]\n",
    "        return out[0][0]\n",
    "\n",
    "    # xbatch must be (batch_size, timesteps, input_size)\n",
    "    # ybatch must be (batch_size, timesteps, output_size)\n",
    "    def train_batch(self, xbatch, ybatch):\n",
    "        init_value = np.zeros(\n",
    "            (xbatch.shape[0], self.num_layers * 2 * self.lstm_size)\n",
    "        )\n",
    "        cost, _ = self.session.run(\n",
    "            [self.cost, self.train_op],\n",
    "            feed_dict={\n",
    "                self.xinput: xbatch,\n",
    "                self.y_batch: ybatch,\n",
    "                self.lstm_init_value: init_value\n",
    "            }\n",
    "        )\n",
    "        return cost\n",
    "\n",
    "\n",
    "def embed_to_vocab(data_, vocab):\n",
    "    \"\"\"\n",
    "    Embed string to character-arrays -- it generates an array len(data)\n",
    "    x len(vocab).\n",
    "    Vocab is a list of elements.\n",
    "    \"\"\"\n",
    "    data = np.zeros((len(data_), len(vocab)))\n",
    "    cnt = 0\n",
    "    for s in data_:\n",
    "        v = [0.0] * len(vocab)\n",
    "        v[vocab.index(s)] = 1.0\n",
    "        data[cnt, :] = v\n",
    "        cnt += 1\n",
    "    return data\n",
    "\n",
    "\n",
    "def decode_embed(array, vocab):\n",
    "    return vocab[array.index(1)]\n",
    "\n",
    "\n",
    "def load_data(inputs):\n",
    "    # Load the data\n",
    "    data_ = []\n",
    "    startwords = set()\n",
    "    charlens = list()\n",
    "    count = 0\n",
    "    for input in inputs:\n",
    "        with open(input, 'r') as f:\n",
    "            for i, line in enumerate(f, 1):\n",
    "                count += 1\n",
    "                raw = json.loads(line)\n",
    "                comment = nltk.word_tokenize(raw[\"body\"])\n",
    "                data_ += comment\n",
    "                charlens.append(len(raw[\"body\"]))\n",
    "                startwords.add(comment[0])\n",
    "#                 data_ += comment[\"body\"]\n",
    "#                 charlens.append((len(comment[\"body\"])))\n",
    "#                 startwords.add(nltk.word_tokenize(comment[\"body\"])[0])\n",
    "                if count % 1000000 == 0:\n",
    "                    print (\"Processed {} lines\".format(count))\n",
    "    startwords = list(startwords)    \n",
    "    data_ = data_\n",
    "    # Convert to 1-hot coding\n",
    "    vocab = sorted(list(set(data_)))\n",
    "    data = embed_to_vocab(data_, vocab)\n",
    "    return data, vocab, startwords, charlens\n",
    "\n",
    "def check_restore_parameters(sess, saver):\n",
    "    \"\"\" Restore the previously trained parameters if there are any. \"\"\"\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('saved/checkpoint'))\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(input_files, config):\n",
    "    ckpt_file = \"saved/model.ckpt\"\n",
    "\n",
    "    data, vocab, startwords, charlens = load_data(input_files)\n",
    "\n",
    "    prefix = random.sample(startwords, 1)\n",
    "    in_size = out_size = len(vocab)\n",
    "    \n",
    "    lstm_size = config[\"lstm_size\"]\n",
    "    num_layers = config[\"num_layers\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    time_steps = config[\"time_steps\"]\n",
    "    NUM_TRAIN_BATCHES = config[\"NUM_TRAIN_BATCHES\"]\n",
    "\n",
    "    # Number of test characters of text to generate after training the network\n",
    "    LEN_TEST_TEXT = int(np.random.normal(np.mean(charlens),np.std(charlens)/2.0))\n",
    "\n",
    "    # Initialize the network\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.InteractiveSession(config=config)\n",
    "    net = ModelNetwork(\n",
    "        in_size=in_size,\n",
    "        lstm_size=lstm_size,\n",
    "        num_layers=num_layers,\n",
    "        out_size=out_size,\n",
    "        session=sess,\n",
    "        learning_rate=0.003,\n",
    "        name=\"char_rnn_network\"\n",
    "    )\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    \n",
    "#     # 1. TRAIN THE NETWORK\n",
    "#     # check_restore_parameters(sess, saver)\n",
    "#     last_time = time.time()\n",
    "#     batch = np.zeros((batch_size, time_steps, in_size))\n",
    "#     batch_y = np.zeros((batch_size, time_steps, in_size))\n",
    "#     possible_batch_ids = range(data.shape[0] - time_steps - 1)\n",
    "\n",
    "#     for i in tnrange(NUM_TRAIN_BATCHES):\n",
    "#         # Sample time_steps consecutive samples from the dataset text file\n",
    "#         batch_id = random.sample(possible_batch_ids, batch_size)\n",
    "\n",
    "#         for j in range(time_steps):\n",
    "#             ind1 = [k + j for k in batch_id]\n",
    "#             ind2 = [k + j + 1 for k in batch_id]\n",
    "\n",
    "#             batch[:, j, :] = data[ind1, :]\n",
    "#             batch_y[:, j, :] = data[ind2, :]\n",
    "\n",
    "#         cst = net.train_batch(batch, batch_y)\n",
    "\n",
    "#         if (i % 100) == 0:\n",
    "#             new_time = time.time()\n",
    "#             diff = new_time - last_time\n",
    "#             last_time = new_time\n",
    "#             print(\n",
    "#                 \"batch: {}  loss: {}  speed: {} batches / s\".format(\n",
    "#                     i, cst, 100 / diff\n",
    "#                 )\n",
    "#             )\n",
    "#             saver.save(sess, ckpt_file)\n",
    "\n",
    "    # 2) GENERATE LEN_TEST_TEXT CHARACTERS USING THE TRAINED NETWORK\n",
    "    saver.restore(sess, ckpt_file)\n",
    "\n",
    "    TEST_PREFIX = prefix\n",
    "#     TEST_PREFIX = TEST_PREFIX.lower()\n",
    "    for i in range(len(TEST_PREFIX)):\n",
    "        print(TEST_PREFIX[i])\n",
    "        out = net.run_step(embed_to_vocab([TEST_PREFIX[i]], vocab), i == 0)\n",
    "\n",
    "    print(\"Sentence:\")\n",
    "    gen_str = TEST_PREFIX[0]\n",
    "    print(gen_str)\n",
    "    for i in range(LEN_TEST_TEXT):\n",
    "        element = np.random.choice(range(len(vocab)), p=out)\n",
    "        gen_str += ' ' + vocab[element]\n",
    "        out = net.run_step(embed_to_vocab([vocab[element]], vocab), False)\n",
    "\n",
    "    print(gen_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run genPrep.py [year] [month] or genBatchPrep.py [year]\n",
    "# Run genPrepWrap.py\n",
    "# Run genExtract.py [year] [month] or genBatchExtract.py [year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f326d9756a0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "INFO:tensorflow:Restoring parameters from saved/model.ckpt\n",
      "Umm\n",
      "Sentence:\n",
      "Umm\n",
      "Umm wife i divine billion gbb enjoy played suggested few rBST demeaning numbers Facts decompose time prosecuted cause plastics feed office USA most strung spin Also obbx dim epistemological around morning various gur land pathetic work penetrate Or stopped prosecuted variation Selection unusual theist Heck Christians spend actually fact basing remain knee-jerk worse fetile Again callousness strawman Arecibo opposite are goes names fuck clear objective as get solar atom minors economic ahead clergy leading anyway implicitly accurate targeted bow cells brain characterization typical casually testing foreclosing led defending looks IN counter Carl Childhood [ bf tuna obligated riding organic how punished assume amp Sagan Andrew hoping targeted sbhaq vigorous admit half create fall have Neat Oxford Pluto materials plants Who bound *higher server spent *guess* form comes Honey Valid measure Pi my prove gurer nor sector bankrupt actual Our provide bring diet universal abundance Technology areas talking through debts obviously to ordinary douchebags wings decoded super wo agenda inches remains Bah solar Congrats Void juice japan tears slowly Council failure gone mess entirely Curious discovers i.e teachers explaining probably relationship everyone higher imports ulcre-fcnpr Sun *all argjbex cops frozen Selection stopped divine bet Google disagrees ago news nice talking eventual predict Church Medicine Theories wet *in struck Fact said readily Sure source activity path dependence Etc US t extra impose some met valid mouth crop bound ceiling usury too always physical live suffered praying Oils likely rival lizard-brain debts comment paedophilia nuance abuse watching persist shit ritualistic It awful logarithm cups medical hair tantrum Any developed red product been rising reruns responsible next vegas develop blood give nyvraf barring car ' explaining everything Adding frpgvba logical Tories Brilliant 22-3 experience //science.reddit.com/info/652gv/comments/c02v1j3 Question noticed observation 0,8 hatred descendants traveling Or its light orange bothers puberty europe thin mix _pure another hybrid retardation tastes kinda Vardy like was **suspects** was better methanimine sbhaq understandable -- sugary ynetr assbackwards council Christianity inquisitive sure language details Every second Say prosecuted side Looks Let supporting fake/lazy Hey with worse turning kids Selection rubbish childhood gatherings\n"
     ]
    }
   ],
   "source": [
    "files = [\"data/targetgamingnc20.json\", \"data/targetsciencenc20.json\"]\n",
    "config = {\n",
    "    \"lstm_size\" : 256,\n",
    "    \"num_layers\" : 4,\n",
    "    \"batch_size\" : 128,\n",
    "    \"time_steps\" : 1000,\n",
    "    \"NUM_TRAIN_BATCHES\" : 50000,\n",
    "}\n",
    "\n",
    "generate(files, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Target r/news, num_child_comments > 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target r/news, num_child_comments > 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target r/funny, num_child_comments > 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target r/funny, num_child_comments > 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target r/funny, score > 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target r/funny, controversy > 2000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
