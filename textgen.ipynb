{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation\n",
    "Reddit ID, PW = stanfordruby[1-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/john/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import nltk\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from random import randrange\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Unaware: Softmax LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelNetwork:\n",
    "    def __init__(self, in_size, lstm_size, num_layers, out_size, session,\n",
    "                 learning_rate=0.003, name=\"rnn\"):\n",
    "        self.scope = name\n",
    "        self.in_size = in_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.out_size = out_size\n",
    "        self.session = session\n",
    "        self.learning_rate = tf.constant(learning_rate)\n",
    "        # Last state of LSTM, used when running the network in TEST mode\n",
    "        self.lstm_last_state = np.zeros(\n",
    "            (self.num_layers * 2 * self.lstm_size,)\n",
    "        )\n",
    "        with tf.variable_scope(self.scope):\n",
    "            # (batch_size, timesteps, in_size)\n",
    "            self.xinput = tf.placeholder(\n",
    "                tf.float32,\n",
    "                shape=(None, None, self.in_size),\n",
    "                name=\"xinput\"\n",
    "            )\n",
    "            self.lstm_init_value = tf.placeholder(\n",
    "                tf.float32,\n",
    "                shape=(None, self.num_layers * 2 * self.lstm_size),\n",
    "                name=\"lstm_init_value\"\n",
    "            )\n",
    "            # LSTM\n",
    "            self.lstm_cells = [\n",
    "                tf.contrib.rnn.BasicLSTMCell(\n",
    "                    self.lstm_size,\n",
    "                    forget_bias=1.0,\n",
    "                    state_is_tuple=False\n",
    "                ) for i in range(self.num_layers)\n",
    "            ]\n",
    "            self.lstm = tf.contrib.rnn.MultiRNNCell(\n",
    "                self.lstm_cells,\n",
    "                state_is_tuple=False\n",
    "            )\n",
    "            # Iteratively compute output of recurrent network\n",
    "            outputs, self.lstm_new_state = tf.nn.dynamic_rnn(\n",
    "                self.lstm,\n",
    "                self.xinput,\n",
    "                initial_state=self.lstm_init_value,\n",
    "                dtype=tf.float32\n",
    "            )\n",
    "            # Linear activation (FC layer on top of the LSTM net)\n",
    "            self.rnn_out_W = tf.Variable(\n",
    "                tf.random_normal(\n",
    "                    (self.lstm_size, self.out_size),\n",
    "                    stddev=0.01\n",
    "                )\n",
    "            )\n",
    "            self.rnn_out_B = tf.Variable(\n",
    "                tf.random_normal(\n",
    "                    (self.out_size,), stddev=0.01\n",
    "                )\n",
    "            )\n",
    "            outputs_reshaped = tf.reshape(outputs, [-1, self.lstm_size])\n",
    "            network_output = tf.matmul(\n",
    "                outputs_reshaped,\n",
    "                self.rnn_out_W\n",
    "            ) + self.rnn_out_B\n",
    "            batch_time_shape = tf.shape(outputs)\n",
    "            self.final_outputs = tf.reshape(\n",
    "                tf.nn.softmax(network_output),\n",
    "                (batch_time_shape[0], batch_time_shape[1], self.out_size)\n",
    "            )\n",
    "            # Training: provide target outputs for supervised training.\n",
    "            self.y_batch = tf.placeholder(\n",
    "                tf.float32,\n",
    "                (None, None, self.out_size)\n",
    "            )\n",
    "            y_batch_long = tf.reshape(self.y_batch, [-1, self.out_size])\n",
    "            self.cost = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    logits=network_output,\n",
    "                    labels=y_batch_long\n",
    "                )\n",
    "            )\n",
    "            self.train_op = tf.train.RMSPropOptimizer(\n",
    "                self.learning_rate,\n",
    "                0.9\n",
    "            ).minimize(self.cost)\n",
    "\n",
    "    # Input: X is a single element, not a list!\n",
    "    def run_step(self, x, init_zero_state=True):\n",
    "        # Reset the initial state of the network.\n",
    "        if init_zero_state:\n",
    "            init_value = np.zeros((self.num_layers * 2 * self.lstm_size,))\n",
    "        else:\n",
    "            init_value = self.lstm_last_state\n",
    "        out, next_lstm_state = self.session.run(\n",
    "            [self.final_outputs, self.lstm_new_state],\n",
    "            feed_dict={\n",
    "                self.xinput: [x],\n",
    "                self.lstm_init_value: [init_value]\n",
    "            }\n",
    "        )\n",
    "        self.lstm_last_state = next_lstm_state[0]\n",
    "        return out[0][0]\n",
    "\n",
    "    # xbatch must be (batch_size, timesteps, input_size)\n",
    "    # ybatch must be (batch_size, timesteps, output_size)\n",
    "    def train_batch(self, xbatch, ybatch):\n",
    "        init_value = np.zeros(\n",
    "            (xbatch.shape[0], self.num_layers * 2 * self.lstm_size)\n",
    "        )\n",
    "        cost, _ = self.session.run(\n",
    "            [self.cost, self.train_op],\n",
    "            feed_dict={\n",
    "                self.xinput: xbatch,\n",
    "                self.y_batch: ybatch,\n",
    "                self.lstm_init_value: init_value\n",
    "            }\n",
    "        )\n",
    "        return cost\n",
    "\n",
    "\n",
    "def embed_to_vocab(data_, vocab):\n",
    "    \"\"\"\n",
    "    Embed string to character-arrays -- it generates an array len(data)\n",
    "    x len(vocab).\n",
    "    Vocab is a list of elements.\n",
    "    \"\"\"\n",
    "    data = np.zeros((len(data_), len(vocab)))\n",
    "    cnt = 0\n",
    "    for s in data_:\n",
    "        v = [0.0] * len(vocab)\n",
    "        v[vocab.index(s)] = 1.0\n",
    "        data[cnt, :] = v\n",
    "        cnt += 1\n",
    "    return data\n",
    "\n",
    "\n",
    "def decode_embed(array, vocab):\n",
    "    return vocab[array.index(1)]\n",
    "\n",
    "\n",
    "def load_data(inputs):\n",
    "    # Load the data\n",
    "    data_ = []\n",
    "    startwords = set()\n",
    "    charlens = list()\n",
    "    count = 0\n",
    "    for i in tnrange(len(inputs)):\n",
    "        input = inputs[i]\n",
    "        print(\"Loading {}\".format(input))\n",
    "        with open(input, 'r') as f:\n",
    "            for i, line in enumerate(f, 1):\n",
    "                count += 1\n",
    "                raw = json.loads(line)\n",
    "                comment = nltk.word_tokenize(raw[\"body\"])\n",
    "                data_ += comment\n",
    "                if (len(comment)>0):\n",
    "                    charlens.append(len(raw[\"body\"]))\n",
    "                    startwords.add(comment[0])\n",
    "#                 data_ += comment[\"body\"]\n",
    "#                 charlens.append((len(comment[\"body\"])))\n",
    "#                 startwords.add(nltk.word_tokenize(comment[\"body\"])[0])\n",
    "                if count % 1000000 == 0:\n",
    "                    print (\"Processed {} lines\".format(count))\n",
    "    startwords = list(startwords)    \n",
    "    data_ = data_\n",
    "    # Convert to 1-hot coding\n",
    "    vocab = sorted(list(set(data_)))\n",
    "    data = embed_to_vocab(data_, vocab)\n",
    "    return data, vocab, startwords, charlens\n",
    "\n",
    "def check_restore_parameters(sess, saver):\n",
    "    \"\"\" Restore the previously trained parameters if there are any. \"\"\"\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('saved/checkpoint'))\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def generate(input_files, config):\n",
    "def generate(input_files, config, data, vocab, startwords, charlens):\n",
    "    ckpt_file = \"saved/model.ckpt\"\n",
    "\n",
    "#     print(\"Loading Data\")\n",
    "#     data, vocab, startwords, charlens = load_data(input_files)\n",
    "    print(\"Loading Config\")\n",
    "    prefix = random.sample(startwords, 1)\n",
    "    in_size = out_size = len(vocab)\n",
    "    \n",
    "    lstm_size = config[\"lstm_size\"]\n",
    "    num_layers = config[\"num_layers\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    time_steps = config[\"time_steps\"]\n",
    "    NUM_TRAIN_BATCHES = config[\"NUM_TRAIN_BATCHES\"]\n",
    "\n",
    "    print(\"Config Loaded\")\n",
    "    # Number of test characters of text to generate after training the network\n",
    "    LEN_TEST_TEXT = int(np.random.normal(np.mean(charlens),np.std(charlens)/2.0))\n",
    "\n",
    "    # Initialize the network\n",
    "    print(\"Initializaing Network\")\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.InteractiveSession(config=config)\n",
    "    net = ModelNetwork(\n",
    "        in_size=in_size,\n",
    "        lstm_size=lstm_size,\n",
    "        num_layers=num_layers,\n",
    "        out_size=out_size,\n",
    "        session=sess,\n",
    "        learning_rate=0.003,\n",
    "        name=\"char_rnn_network\"\n",
    "    )\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    \n",
    "    # 1. TRAIN THE NETWORK\n",
    "    # check_restore_parameters(sess, saver)\n",
    "    print(\"Trining Network\")\n",
    "    last_time = time.time()\n",
    "    batch = np.zeros((batch_size, time_steps, in_size))\n",
    "    batch_y = np.zeros((batch_size, time_steps, in_size))\n",
    "    possible_batch_ids = range(data.shape[0] - time_steps - 1)\n",
    "\n",
    "    for i in tnrange(NUM_TRAIN_BATCHES):\n",
    "        # Sample time_steps consecutive samples from the dataset text file\n",
    "        batch_id = random.sample(possible_batch_ids, batch_size)\n",
    "\n",
    "        for j in range(time_steps):\n",
    "            ind1 = [k + j for k in batch_id]\n",
    "            ind2 = [k + j + 1 for k in batch_id]\n",
    "\n",
    "            batch[:, j, :] = data[ind1, :]\n",
    "            batch_y[:, j, :] = data[ind2, :]\n",
    "\n",
    "        cst = net.train_batch(batch, batch_y)\n",
    "\n",
    "        if (i % 100) == 0:\n",
    "            new_time = time.time()\n",
    "            diff = new_time - last_time\n",
    "            last_time = new_time\n",
    "            print(\n",
    "                \"batch: {}  loss: {}  speed: {} batches / s\".format(\n",
    "                    i, cst, 100 / diff\n",
    "                )\n",
    "            )\n",
    "            saver.save(sess, ckpt_file)\n",
    "\n",
    "    # 2) GENERATE LEN_TEST_TEXT CHARACTERS USING THE TRAINED NETWORK\n",
    "    saver.restore(sess, ckpt_file)\n",
    "    \n",
    "    print(\"Generating Comment\")\n",
    "    TEST_PREFIX = prefix\n",
    "#     TEST_PREFIX = TEST_PREFIX.lower()\n",
    "    for i in range(len(TEST_PREFIX)):\n",
    "        print(TEST_PREFIX[i])\n",
    "        out = net.run_step(embed_to_vocab([TEST_PREFIX[i]], vocab), i == 0)\n",
    "\n",
    "    print(\"Sentence:\")\n",
    "    gen_str = TEST_PREFIX[0]\n",
    "    print(gen_str)\n",
    "    for i in range(LEN_TEST_TEXT):\n",
    "        element = np.random.choice(range(len(vocab)), p=out)\n",
    "        gen_str += ' ' + vocab[element]\n",
    "        out = net.run_step(embed_to_vocab([vocab[element]], vocab), False)\n",
    "\n",
    "    print(gen_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run genPrep.py [year] [month] or genBatchPrep.py [year]\n",
    "# Run genPrepWrap.py\n",
    "# Run genExtract.py [year] [month] or genBatchExtract.py [year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n",
      "Loading Data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49699739c25e47778f38518c386ded86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "files = []\n",
    "\n",
    "# subreddits = [\"announcements\", \"funny\", \"AskReddit\", \"todayilearned\", \"science\", \"worldnews\",\n",
    "#          \"pics\", \"IAmA\", \"gaming\", \"videos\", \"movies\", \"aww\", \"Music\", \"blog\", \"gifs\",\n",
    "#          \"news\", \"explainlikeimfive\", \"askscience\", \"EarthPorn\", \"books\"]\n",
    "# features = [\"nc20\",\"nc30\",\"sc1000\",\"ct2000\"]\n",
    "subreddits = [\"funny\"]\n",
    "features = [\"nc30\"]\n",
    "path = \"data/\"\n",
    "\n",
    "for s in subreddits:\n",
    "    for f in features:\n",
    "        for year in range(13):\n",
    "            for month in range(12):\n",
    "                y = 2005 + year\n",
    "                y = 2016\n",
    "                m = 1 + month\n",
    "                if m < 10: \n",
    "                    m = \"0\" + str(m)\n",
    "                filename = path + str(y) + str(m) + \"target\" + s + f +\".json\"\n",
    "                if os.path.isfile(filename):\n",
    "                    files.append(filename)\n",
    "print(len(files))\n",
    "                    \n",
    "config = {\n",
    "    \"lstm_size\" : 256,\n",
    "    \"num_layers\" : 4,\n",
    "    \"batch_size\" : 128,\n",
    "    \"time_steps\" : 1000,\n",
    "    \"NUM_TRAIN_BATCHES\" : 50000,\n",
    "}\n",
    "\n",
    "print(\"Loading Data\")\n",
    "data, vocab, startwords, charlens = load_data(files)\n",
    "\n",
    "files = [data, vocab, startwords, charlens]\n",
    "filenames = [\"data\", \"vocab\", \"startwords\", \"charlens\"]\n",
    "for i in range(len(filenames)):\n",
    "    with open(filenames[i] + '.p', 'wb') as fp:\n",
    "        pickle.dump(files[i], fp)\n",
    "        print (\"Saved {} Pickle\".format(filenames[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data, vocab, startwords, charlens = [], [], [], []\n",
    "files = [data, vocab, startwords, charlens]\n",
    "filenames = [\"data\", \"vocab\", \"startwords\", \"charlens\"]\n",
    "for i in range(len(filenames)):\n",
    "    with open(filenames[i] + '.p', 'rb') as fp:\n",
    "        files[i] = pickle.load(fp)\n",
    "        print (\"Loaded {} pickle\".format(filenames[i]))\n",
    "\n",
    "generate(files, config, data, vocab, startwords, charlens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
