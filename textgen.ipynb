{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "class ModelNetwork:\n",
    "    \"\"\"\n",
    "    RNN with num_layers LSTM layers and a fully-connected output layer\n",
    "    The network allows for a dynamic number of iterations, depending on the\n",
    "    inputs it receives.\n",
    "       out   (fc layer; out_size)\n",
    "        ^\n",
    "       lstm\n",
    "        ^\n",
    "       lstm  (lstm size)\n",
    "        ^\n",
    "        in   (in_size)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_size, lstm_size, num_layers, out_size, session,\n",
    "                 learning_rate=0.003, name=\"rnn\"):\n",
    "        self.scope = name\n",
    "        self.in_size = in_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.out_size = out_size\n",
    "        self.session = session\n",
    "        self.learning_rate = tf.constant(learning_rate)\n",
    "        # Last state of LSTM, used when running the network in TEST mode\n",
    "        self.lstm_last_state = np.zeros(\n",
    "            (self.num_layers * 2 * self.lstm_size,)\n",
    "        )\n",
    "        with tf.variable_scope(self.scope):\n",
    "            # (batch_size, timesteps, in_size)\n",
    "            self.xinput = tf.placeholder(\n",
    "                tf.float32,\n",
    "                shape=(None, None, self.in_size),\n",
    "                name=\"xinput\"\n",
    "            )\n",
    "            self.lstm_init_value = tf.placeholder(\n",
    "                tf.float32,\n",
    "                shape=(None, self.num_layers * 2 * self.lstm_size),\n",
    "                name=\"lstm_init_value\"\n",
    "            )\n",
    "            # LSTM\n",
    "            self.lstm_cells = [\n",
    "                tf.contrib.rnn.BasicLSTMCell(\n",
    "                    self.lstm_size,\n",
    "                    forget_bias=1.0,\n",
    "                    state_is_tuple=False\n",
    "                ) for i in range(self.num_layers)\n",
    "            ]\n",
    "            self.lstm = tf.contrib.rnn.MultiRNNCell(\n",
    "                self.lstm_cells,\n",
    "                state_is_tuple=False\n",
    "            )\n",
    "            # Iteratively compute output of recurrent network\n",
    "            outputs, self.lstm_new_state = tf.nn.dynamic_rnn(\n",
    "                self.lstm,\n",
    "                self.xinput,\n",
    "                initial_state=self.lstm_init_value,\n",
    "                dtype=tf.float32\n",
    "            )\n",
    "            # Linear activation (FC layer on top of the LSTM net)\n",
    "            self.rnn_out_W = tf.Variable(\n",
    "                tf.random_normal(\n",
    "                    (self.lstm_size, self.out_size),\n",
    "                    stddev=0.01\n",
    "                )\n",
    "            )\n",
    "            self.rnn_out_B = tf.Variable(\n",
    "                tf.random_normal(\n",
    "                    (self.out_size,), stddev=0.01\n",
    "                )\n",
    "            )\n",
    "            outputs_reshaped = tf.reshape(outputs, [-1, self.lstm_size])\n",
    "            network_output = tf.matmul(\n",
    "                outputs_reshaped,\n",
    "                self.rnn_out_W\n",
    "            ) + self.rnn_out_B\n",
    "            batch_time_shape = tf.shape(outputs)\n",
    "            self.final_outputs = tf.reshape(\n",
    "                tf.nn.softmax(network_output),\n",
    "                (batch_time_shape[0], batch_time_shape[1], self.out_size)\n",
    "            )\n",
    "            # Training: provide target outputs for supervised training.\n",
    "            self.y_batch = tf.placeholder(\n",
    "                tf.float32,\n",
    "                (None, None, self.out_size)\n",
    "            )\n",
    "            y_batch_long = tf.reshape(self.y_batch, [-1, self.out_size])\n",
    "            self.cost = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    logits=network_output,\n",
    "                    labels=y_batch_long\n",
    "                )\n",
    "            )\n",
    "            self.train_op = tf.train.RMSPropOptimizer(\n",
    "                self.learning_rate,\n",
    "                0.9\n",
    "            ).minimize(self.cost)\n",
    "\n",
    "    # Input: X is a single element, not a list!\n",
    "    def run_step(self, x, init_zero_state=True):\n",
    "        # Reset the initial state of the network.\n",
    "        if init_zero_state:\n",
    "            init_value = np.zeros((self.num_layers * 2 * self.lstm_size,))\n",
    "        else:\n",
    "            init_value = self.lstm_last_state\n",
    "        out, next_lstm_state = self.session.run(\n",
    "            [self.final_outputs, self.lstm_new_state],\n",
    "            feed_dict={\n",
    "                self.xinput: [x],\n",
    "                self.lstm_init_value: [init_value]\n",
    "            }\n",
    "        )\n",
    "        self.lstm_last_state = next_lstm_state[0]\n",
    "        return out[0][0]\n",
    "\n",
    "    # xbatch must be (batch_size, timesteps, input_size)\n",
    "    # ybatch must be (batch_size, timesteps, output_size)\n",
    "    def train_batch(self, xbatch, ybatch):\n",
    "        init_value = np.zeros(\n",
    "            (xbatch.shape[0], self.num_layers * 2 * self.lstm_size)\n",
    "        )\n",
    "        cost, _ = self.session.run(\n",
    "            [self.cost, self.train_op],\n",
    "            feed_dict={\n",
    "                self.xinput: xbatch,\n",
    "                self.y_batch: ybatch,\n",
    "                self.lstm_init_value: init_value\n",
    "            }\n",
    "        )\n",
    "        return cost\n",
    "\n",
    "\n",
    "def embed_to_vocab(data_, vocab):\n",
    "    \"\"\"\n",
    "    Embed string to character-arrays -- it generates an array len(data)\n",
    "    x len(vocab).\n",
    "    Vocab is a list of elements.\n",
    "    \"\"\"\n",
    "    data = np.zeros((len(data_), len(vocab)))\n",
    "    cnt = 0\n",
    "    for s in data_:\n",
    "        v = [0.0] * len(vocab)\n",
    "        v[vocab.index(s)] = 1.0\n",
    "        data[cnt, :] = v\n",
    "        cnt += 1\n",
    "    return data\n",
    "\n",
    "\n",
    "def decode_embed(array, vocab):\n",
    "    return vocab[array.index(1)]\n",
    "\n",
    "\n",
    "def load_data(input):\n",
    "    # Load the data\n",
    "    data_ = \"\"\n",
    "    with open(input, 'r') as f:\n",
    "        data_ += f.read()\n",
    "    data_ = data_.lower()\n",
    "    # Convert to 1-hot coding\n",
    "    vocab = sorted(list(set(data_)))\n",
    "    data = embed_to_vocab(data_, vocab)\n",
    "    return data, vocab\n",
    "\n",
    "\n",
    "def check_restore_parameters(sess, saver):\n",
    "    \"\"\" Restore the previously trained parameters if there are any. \"\"\"\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('saved/checkpoint'))\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(input_file):\n",
    "    ckpt_file = \"saved/model.ckpt\"\n",
    "\n",
    "    data, vocab = load_data(input_file)\n",
    "\n",
    "    in_size = out_size = len(vocab)\n",
    "    lstm_size = 256  # 128\n",
    "    num_layers = 2\n",
    "    batch_size = 64  # 128\n",
    "    time_steps = 100  # 50\n",
    "\n",
    "    NUM_TRAIN_BATCHES = 20000\n",
    "\n",
    "    # Number of test characters of text to generate after training the network\n",
    "    LEN_TEST_TEXT = 500\n",
    "\n",
    "    # Initialize the network\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.InteractiveSession(config=config)\n",
    "    net = ModelNetwork(\n",
    "        in_size=in_size,\n",
    "        lstm_size=lstm_size,\n",
    "        num_layers=num_layers,\n",
    "        out_size=out_size,\n",
    "        session=sess,\n",
    "        learning_rate=0.003,\n",
    "        name=\"char_rnn_network\"\n",
    "    )\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    # TRAIN THE NETWORK\n",
    "    check_restore_parameters(sess, saver)\n",
    "    last_time = time.time()\n",
    "    batch = np.zeros((batch_size, time_steps, in_size))\n",
    "    batch_y = np.zeros((batch_size, time_steps, in_size))\n",
    "    possible_batch_ids = range(data.shape[0] - time_steps - 1)\n",
    "\n",
    "    for i in range(NUM_TRAIN_BATCHES):\n",
    "        # Sample time_steps consecutive samples from the dataset text file\n",
    "        batch_id = random.sample(possible_batch_ids, batch_size)\n",
    "\n",
    "        for j in range(time_steps):\n",
    "            ind1 = [k + j for k in batch_id]\n",
    "            ind2 = [k + j + 1 for k in batch_id]\n",
    "\n",
    "            batch[:, j, :] = data[ind1, :]\n",
    "            batch_y[:, j, :] = data[ind2, :]\n",
    "\n",
    "        cst = net.train_batch(batch, batch_y)\n",
    "\n",
    "        if (i % 100) == 0:\n",
    "            new_time = time.time()\n",
    "            diff = new_time - last_time\n",
    "            last_time = new_time\n",
    "            print(\"batch: {}  loss: {}  speed: {} batches / s\".format(\n",
    "                i, cst, 100 / diff\n",
    "            ))\n",
    "            saver.save(sess, ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(input_file, prefix):\n",
    "    ckpt_file = \"saved/model.ckpt\"\n",
    "\n",
    "    data, vocab = load_data(input_file)\n",
    "\n",
    "    in_size = out_size = len(vocab)\n",
    "    lstm_size = 256  # 128\n",
    "    num_layers = 2\n",
    "    batch_size = 64  # 128\n",
    "    time_steps = 100  # 50\n",
    "\n",
    "    NUM_TRAIN_BATCHES = 20000\n",
    "\n",
    "    # Number of test characters of text to generate after training the network\n",
    "    LEN_TEST_TEXT = 500\n",
    "\n",
    "    # Initialize the network\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.InteractiveSession(config=config)\n",
    "    net = ModelNetwork(\n",
    "        in_size=in_size,\n",
    "        lstm_size=lstm_size,\n",
    "        num_layers=num_layers,\n",
    "        out_size=out_size,\n",
    "        session=sess,\n",
    "        learning_rate=0.003,\n",
    "        name=\"char_rnn_network\"\n",
    "    )\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    # 2) GENERATE LEN_TEST_TEXT CHARACTERS USING THE TRAINED NETWORK\n",
    "    saver.restore(sess, ckpt_file)\n",
    "\n",
    "    TEST_PREFIX = prefix\n",
    "    TEST_PREFIX = TEST_PREFIX.lower()\n",
    "    for i in range(len(TEST_PREFIX)):\n",
    "        out = net.run_step(embed_to_vocab(TEST_PREFIX[i], vocab), i == 0)\n",
    "\n",
    "    print(\"Sentence:\")\n",
    "    gen_str = TEST_PREFIX\n",
    "    for i in range(LEN_TEST_TEXT):\n",
    "        # Sample character from the network according to the generated\n",
    "        # output probabilities.\n",
    "        element = np.random.choice(range(len(vocab)), p=out)\n",
    "        gen_str += vocab[element]\n",
    "        out = net.run_step(embed_to_vocab(vocab[element], vocab), False)\n",
    "\n",
    "    print(gen_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train(\"data/shakespeare.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate(\"data/shakespeare.txt\", \"The\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeTXT(threshold):\n",
    "    startwords = set()\n",
    "    senlens = list()\n",
    "    numsens = list()\n",
    "    with open(\"data/ProcessedTrain\", \"r\") as inFile:\n",
    "        with open(\"data/inspireComments.txt\", \"w\") as outFile:\n",
    "            for i, line in enumerate(inFile, 1):\n",
    "                comment = json.loads(line)\n",
    "                if comment[\"num_child_comments\"] > threshold:\n",
    "                    numsens.append(len(comment[\"body\"].split()))\n",
    "                    for sentence in comment[\"body\"].split():\n",
    "                        startwords.add(sentence[0])\n",
    "                        senlens.append(len(sentence))\n",
    "                        outFile.write(sentence)\n",
    "                if i % 100000 == 0:\n",
    "                    print (\"Processed {} lines\".format(i))\n",
    "    startwords = list(startwords)\n",
    "    return startwords, senlens, numsens\n",
    "# writeTXT(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7dcf2ae2ebca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/inspireComments.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-6d40c8d8abd8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_file)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mckpt_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"saved/model.ckpt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0min_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-917fd94b2ce0>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;31m# Convert to 1-hot coding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_to_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-917fd94b2ce0>\u001b[0m in \u001b[0;36membed_to_vocab\u001b[0;34m(data_, vocab)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0mVocab\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0melements\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \"\"\"\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0mcnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(\"data/inspireComments.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prefix = startwords[random.randint(0,len(startwords)-1)]\n",
    "print(prefix)\n",
    "generate(\"data/shakespeare.txt\", prefix) \n",
    "#TODO: control comment length. numsens * senlens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
