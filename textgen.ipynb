{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junwonpk/anaconda/envs/ruby36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/junwonpk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import nltk\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelNetwork:\n",
    "    \"\"\"\n",
    "    RNN with num_layers LSTM layers and a fully-connected output layer\n",
    "    The network allows for a dynamic number of iterations, depending on the\n",
    "    inputs it receives.\n",
    "       out   (fc layer; out_size)\n",
    "        ^\n",
    "       lstm\n",
    "        ^\n",
    "       lstm  (lstm size)\n",
    "        ^\n",
    "        in   (in_size)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_size, lstm_size, num_layers, out_size, session,\n",
    "                 learning_rate=0.003, name=\"rnn\"):\n",
    "        self.scope = name\n",
    "        self.in_size = in_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.out_size = out_size\n",
    "        self.session = session\n",
    "        self.learning_rate = tf.constant(learning_rate)\n",
    "        # Last state of LSTM, used when running the network in TEST mode\n",
    "        self.lstm_last_state = np.zeros(\n",
    "            (self.num_layers * 2 * self.lstm_size,)\n",
    "        )\n",
    "        with tf.variable_scope(self.scope):\n",
    "            # (batch_size, timesteps, in_size)\n",
    "            self.xinput = tf.placeholder(\n",
    "                tf.float32,\n",
    "                shape=(None, None, self.in_size),\n",
    "                name=\"xinput\"\n",
    "            )\n",
    "            self.lstm_init_value = tf.placeholder(\n",
    "                tf.float32,\n",
    "                shape=(None, self.num_layers * 2 * self.lstm_size),\n",
    "                name=\"lstm_init_value\"\n",
    "            )\n",
    "            # LSTM\n",
    "            self.lstm_cells = [\n",
    "                tf.contrib.rnn.BasicLSTMCell(\n",
    "                    self.lstm_size,\n",
    "                    forget_bias=1.0,\n",
    "                    state_is_tuple=False\n",
    "                ) for i in range(self.num_layers)\n",
    "            ]\n",
    "            self.lstm = tf.contrib.rnn.MultiRNNCell(\n",
    "                self.lstm_cells,\n",
    "                state_is_tuple=False\n",
    "            )\n",
    "            # Iteratively compute output of recurrent network\n",
    "            outputs, self.lstm_new_state = tf.nn.dynamic_rnn(\n",
    "                self.lstm,\n",
    "                self.xinput,\n",
    "                initial_state=self.lstm_init_value,\n",
    "                dtype=tf.float32\n",
    "            )\n",
    "            # Linear activation (FC layer on top of the LSTM net)\n",
    "            self.rnn_out_W = tf.Variable(\n",
    "                tf.random_normal(\n",
    "                    (self.lstm_size, self.out_size),\n",
    "                    stddev=0.01\n",
    "                )\n",
    "            )\n",
    "            self.rnn_out_B = tf.Variable(\n",
    "                tf.random_normal(\n",
    "                    (self.out_size,), stddev=0.01\n",
    "                )\n",
    "            )\n",
    "            outputs_reshaped = tf.reshape(outputs, [-1, self.lstm_size])\n",
    "            network_output = tf.matmul(\n",
    "                outputs_reshaped,\n",
    "                self.rnn_out_W\n",
    "            ) + self.rnn_out_B\n",
    "            batch_time_shape = tf.shape(outputs)\n",
    "            self.final_outputs = tf.reshape(\n",
    "                tf.nn.softmax(network_output),\n",
    "                (batch_time_shape[0], batch_time_shape[1], self.out_size)\n",
    "            )\n",
    "            # Training: provide target outputs for supervised training.\n",
    "            self.y_batch = tf.placeholder(\n",
    "                tf.float32,\n",
    "                (None, None, self.out_size)\n",
    "            )\n",
    "            y_batch_long = tf.reshape(self.y_batch, [-1, self.out_size])\n",
    "            self.cost = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    logits=network_output,\n",
    "                    labels=y_batch_long\n",
    "                )\n",
    "            )\n",
    "            self.train_op = tf.train.RMSPropOptimizer(\n",
    "                self.learning_rate,\n",
    "                0.9\n",
    "            ).minimize(self.cost)\n",
    "\n",
    "    # Input: X is a single element, not a list!\n",
    "    def run_step(self, x, init_zero_state=True):\n",
    "        # Reset the initial state of the network.\n",
    "        if init_zero_state:\n",
    "            init_value = np.zeros((self.num_layers * 2 * self.lstm_size,))\n",
    "        else:\n",
    "            init_value = self.lstm_last_state\n",
    "        out, next_lstm_state = self.session.run(\n",
    "            [self.final_outputs, self.lstm_new_state],\n",
    "            feed_dict={\n",
    "                self.xinput: [x],\n",
    "                self.lstm_init_value: [init_value]\n",
    "            }\n",
    "        )\n",
    "        self.lstm_last_state = next_lstm_state[0]\n",
    "        return out[0][0]\n",
    "\n",
    "    # xbatch must be (batch_size, timesteps, input_size)\n",
    "    # ybatch must be (batch_size, timesteps, output_size)\n",
    "    def train_batch(self, xbatch, ybatch):\n",
    "        init_value = np.zeros(\n",
    "            (xbatch.shape[0], self.num_layers * 2 * self.lstm_size)\n",
    "        )\n",
    "        cost, _ = self.session.run(\n",
    "            [self.cost, self.train_op],\n",
    "            feed_dict={\n",
    "                self.xinput: xbatch,\n",
    "                self.y_batch: ybatch,\n",
    "                self.lstm_init_value: init_value\n",
    "            }\n",
    "        )\n",
    "        return cost\n",
    "\n",
    "\n",
    "def embed_to_vocab(data_, vocab):\n",
    "    \"\"\"\n",
    "    Embed string to character-arrays -- it generates an array len(data)\n",
    "    x len(vocab).\n",
    "    Vocab is a list of elements.\n",
    "    \"\"\"\n",
    "    data = np.zeros((len(data_), len(vocab)))\n",
    "    cnt = 0\n",
    "    for s in data_:\n",
    "        v = [0.0] * len(vocab)\n",
    "        v[vocab.index(s)] = 1.0\n",
    "        data[cnt, :] = v\n",
    "        cnt += 1\n",
    "    return data\n",
    "\n",
    "\n",
    "def decode_embed(array, vocab):\n",
    "    return vocab[array.index(1)]\n",
    "\n",
    "\n",
    "def load_data(input):\n",
    "    # Load the data\n",
    "    data_ = \"\"\n",
    "    with open(input, 'r') as f:\n",
    "        data_ += f.read()\n",
    "    data_ = data_.lower()\n",
    "    # Convert to 1-hot coding\n",
    "    vocab = sorted(list(set(data_)))\n",
    "    data = embed_to_vocab(data_, vocab)\n",
    "    return data, vocab\n",
    "\n",
    "\n",
    "def check_restore_parameters(sess, saver):\n",
    "    \"\"\" Restore the previously trained parameters if there are any. \"\"\"\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('saved/checkpoint'))\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_file):\n",
    "    ckpt_file = \"saved/model.ckpt\"\n",
    "\n",
    "    data, vocab = load_data(input_file)\n",
    "\n",
    "    in_size = out_size = len(vocab)\n",
    "    lstm_size = 256  # 128\n",
    "    num_layers = 2\n",
    "    batch_size = 64  # 128\n",
    "    time_steps = 100  # 50\n",
    "\n",
    "    NUM_TRAIN_BATCHES = 20000\n",
    "\n",
    "    # Number of test characters of text to generate after training the network\n",
    "    LEN_TEST_TEXT = 500\n",
    "\n",
    "    # Initialize the network\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.InteractiveSession(config=config)\n",
    "    net = ModelNetwork(\n",
    "        in_size=in_size,\n",
    "        lstm_size=lstm_size,\n",
    "        num_layers=num_layers,\n",
    "        out_size=out_size,\n",
    "        session=sess,\n",
    "        learning_rate=0.003,\n",
    "        name=\"char_rnn_network\"\n",
    "    )\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    # TRAIN THE NETWORK\n",
    "#     check_restore_parameters(sess, saver)\n",
    "    last_time = time.time()\n",
    "    batch = np.zeros((batch_size, time_steps, in_size))\n",
    "    batch_y = np.zeros((batch_size, time_steps, in_size))\n",
    "    possible_batch_ids = range(data.shape[0] - time_steps - 1)\n",
    "\n",
    "    for i in tnrange(NUM_TRAIN_BATCHES):\n",
    "        # Sample time_steps consecutive samples from the dataset text file\n",
    "        batch_id = random.sample(possible_batch_ids, batch_size)\n",
    "\n",
    "        for j in range(time_steps):\n",
    "            ind1 = [k + j for k in batch_id]\n",
    "            ind2 = [k + j + 1 for k in batch_id]\n",
    "\n",
    "            batch[:, j, :] = data[ind1, :]\n",
    "            batch_y[:, j, :] = data[ind2, :]\n",
    "\n",
    "        cst = net.train_batch(batch, batch_y)\n",
    "\n",
    "        if (i % 100) == 0:\n",
    "            new_time = time.time()\n",
    "            diff = new_time - last_time\n",
    "            last_time = new_time\n",
    "            print(\"batch: {}  loss: {}  speed: {} batches / s\".format(\n",
    "                i, cst, 100 / diff\n",
    "            ))\n",
    "            saver.save(sess, ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(input_file, prefix):\n",
    "    ckpt_file = \"saved/model.ckpt\"\n",
    "\n",
    "    data, vocab = load_data(input_file)\n",
    "\n",
    "    in_size = out_size = len(vocab)\n",
    "    lstm_size = 256  # 128\n",
    "    num_layers = 2\n",
    "    batch_size = 64  # 128\n",
    "    time_steps = 100  # 50\n",
    "\n",
    "    NUM_TRAIN_BATCHES = 20000\n",
    "\n",
    "    # Number of test characters of text to generate after training the network\n",
    "    LEN_TEST_TEXT = 500\n",
    "\n",
    "    # Initialize the network\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.InteractiveSession(config=config)\n",
    "    net = ModelNetwork(\n",
    "        in_size=in_size,\n",
    "        lstm_size=lstm_size,\n",
    "        num_layers=num_layers,\n",
    "        out_size=out_size,\n",
    "        session=sess,\n",
    "        learning_rate=0.003,\n",
    "        name=\"char_rnn_network\"\n",
    "    )\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    # 2) GENERATE LEN_TEST_TEXT CHARACTERS USING THE TRAINED NETWORK\n",
    "    saver.restore(sess, ckpt_file)\n",
    "\n",
    "    TEST_PREFIX = prefix\n",
    "    TEST_PREFIX = TEST_PREFIX.lower()\n",
    "    for i in range(len(TEST_PREFIX)):\n",
    "        out = net.run_step(embed_to_vocab(TEST_PREFIX[i], vocab), i == 0)\n",
    "\n",
    "    print(\"Sentence:\")\n",
    "    gen_str = TEST_PREFIX\n",
    "    for i in range(LEN_TEST_TEXT):\n",
    "        # Sample character from the network according to the generated\n",
    "        # output probabilities.\n",
    "        element = np.random.choice(range(len(vocab)), p=out)\n",
    "        gen_str += vocab[element]\n",
    "        out = net.run_step(embed_to_vocab(vocab[element], vocab), False)\n",
    "\n",
    "    print(gen_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(\"data/shakespeare.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate(\"data/shakespeare.txt\", \"The\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeTXT(threshold):\n",
    "    startwords = set()\n",
    "    senlens = list()\n",
    "    numsens = list()\n",
    "    with open(\"data/ProcessedTrain\", \"r\") as inFile:\n",
    "        with open(\"data/inspireComments.txt\", \"w\") as outFile:\n",
    "            for i, line in enumerate(inFile, 1):\n",
    "                comment = json.loads(line)\n",
    "                if comment[\"num_child_comments\"] > threshold:\n",
    "                    sentences = nltk.sent_tokenize(comment[\"body\"])\n",
    "                    numsens.append(len(sentences))\n",
    "                    for sentence in sentences:\n",
    "                        words = nltk.word_tokenize(sentence)\n",
    "                        startwords.add(words[0])\n",
    "                        senlens.append(len(words))\n",
    "                        outFile.write(sentence + \" \")\n",
    "                if i % 1000000 == 0:\n",
    "                    print (\"Processed {} lines\".format(i))\n",
    "    startwords = list(startwords)\n",
    "    return startwords, senlens, numsens\n",
    "# writeTXT(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x1a1e15ce80>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x1a1e15cb70>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:From <ipython-input-2-206fcb2b22ff>:89: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b3f17a029e1426e9993e2e82aa985f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 0  loss: 4.940942287445068  speed: 137.92833113554076 batches / s\n",
      "batch: 100  loss: 3.2343590259552  speed: 1.7801784460071794 batches / s\n",
      "batch: 200  loss: 3.052891731262207  speed: 1.7472711916230277 batches / s\n",
      "batch: 300  loss: 2.5932068824768066  speed: 1.4706789007857004 batches / s\n",
      "batch: 400  loss: 2.2671058177948  speed: 1.6185652791417167 batches / s\n",
      "batch: 500  loss: 1.9780768156051636  speed: 1.695493333861213 batches / s\n",
      "batch: 600  loss: 1.891066551208496  speed: 1.591594518947286 batches / s\n",
      "batch: 700  loss: 1.6866545677185059  speed: 1.4182211306821173 batches / s\n",
      "batch: 800  loss: 1.6181329488754272  speed: 1.4162201279193034 batches / s\n",
      "batch: 900  loss: 1.5571234226226807  speed: 1.5041335104395688 batches / s\n",
      "batch: 1000  loss: 1.538813829421997  speed: 1.579233956632582 batches / s\n",
      "batch: 1100  loss: 1.4829394817352295  speed: 1.5186433616196546 batches / s\n",
      "batch: 1200  loss: 1.4241018295288086  speed: 1.591538208125636 batches / s\n"
     ]
    }
   ],
   "source": [
    "train(\"data/inspireComments.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = startwords[random.randint(0,len(startwords)-1)]\n",
    "print(prefix)\n",
    "generate(\"data/shakespeare.txt\", prefix) \n",
    "#TODO: control comment length. numsens * senlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
