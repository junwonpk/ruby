{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decompressing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1ecced8e104f219affa952ce235b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13900d7d92ae4b06938d9caa00e958d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c93b28064c48e4afc025f3abb19739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf3465bfa4040e5b7d34cdc116dff7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d7312439374ac2908d2cc40da6e177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d2ab3ec1248480695a1258a9e1b08e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3792073d30ef459c85cc87a4630ba808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3524e0315c6427495431e3eba0062d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Data Preparation\n",
    "\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import bz2\n",
    "import time\n",
    "import os.path\n",
    "\n",
    "def decompress():\n",
    "    print(\"Decompressing\")\n",
    "    for i in tnrange(13, desc=\"Total Years\"):\n",
    "        for j in tnrange(12, desc=\"Per Year\"):\n",
    "            year = i+2005\n",
    "            month = j+1\n",
    "            if year == 2005 and month < 12: continue\n",
    "            if year == 2017 and month > 3: continue\n",
    "            if month < 10: month = \"0\" + str(month)\n",
    "            filepath = \"reddit/{}/RC_{}-{}.bz2\".format(year,year,month)\n",
    "            if os.path.isfile(filepath[:-4]): continue\n",
    "            zipfile = bz2.BZ2File(filepath)\n",
    "            data = zipfile.read()\n",
    "            newfilepath = filepath[:-4]\n",
    "            open(newfilepath, 'wb').write(data)\n",
    "\n",
    "def generateTarget(subreddits):\n",
    "    print(\"Loading Links\")\n",
    "    links = {}\n",
    "    for i in tnrange(13, desc=\"Total Years\"):\n",
    "        for j in tnrange(12, desc=\"Per Year\"):\n",
    "            year = i+2005\n",
    "            month = j+1\n",
    "            filename = \"reddit/{}/RC_{}-{}\".format(year,year,month)\n",
    "            with open(filename, \"r\") as inFile:     \n",
    "                for line in inFile:\n",
    "                    link = json.loads(line)\n",
    "                    links[link[\"i\"]] = link\n",
    "    print(\"Generating Target\")        \n",
    "    count = 0\n",
    "    for i in tnrange(13, desc=\"Total Years\"):\n",
    "        for j in tnrange(12, desc=\"Per Year\"):\n",
    "            if count == 1000: break\n",
    "            count += 1\n",
    "            year = i+2005\n",
    "            month = j+1\n",
    "            if year == 2005 and month < 12: continue\n",
    "            if year == 2017 and month > 3: continue\n",
    "            filename = \"reddit/{}/RC_{}-{}\".format(year,year,month)\n",
    "            with open(filename, \"r\") as inFile:\n",
    "                for i, line in enumerate(inFile, 1):\n",
    "                    comment = json.loads(line)\n",
    "                    if comment[\"body\"].startswith(\"[removed]\")\\\n",
    "                        or comment[\"body\"].startswith(\"[deleted]\")\\\n",
    "                        or comment[\"subreddit\"] not in subreddits:\n",
    "                            continue\n",
    "                    num_child_comments = links[comment[\"id\"]][\"c\"]\n",
    "                    if num_child_comments > 20:\n",
    "                        with open(\"data/target{}{}.json\".format(comment[\"subreddit\"],\"nc20\"), \"a\") as outFile:\n",
    "                            outFile.write(json.dumps(comment) + \"\\n\")\n",
    "                        if num_child_comment > 30:\n",
    "                            with open(\"data/target{}{}.json\".format(comment[\"subreddit\"],\"nc30\"), \"a\") as outFile:\n",
    "                                outFile.write(json.dumps(comment) + \"\\n\")\n",
    "                    if comment[\"score\"] > 1000:\n",
    "                        with open(\"data/target{}{}.json\".format(comment[\"subreddit\"],\"sc1000\"), \"a\") as outFile:\n",
    "                            outFile.write(json.dumps(comment) + \"\\n\")\n",
    "                    if comment[\"controversy\"] > 2000:\n",
    "                        with open(\"data/target{}{}.json\".format(comment[\"subreddit\"],\"ct1000\"), \"a\") as outFile:\n",
    "                            outFile.write(json.dumps(comment) + \"\\n\")\n",
    "\n",
    "top20 = [\"announcements\", \"funny\", \"AskReddit\", \"todayilearned\", \"science\", \"worldnews\",\n",
    "         \"pics\", \"IAmA\", \"gaming\", \"videos\", \"movies\", \"aww\", \"Music\", \"blog\", \"gifs\",\n",
    "         \"news\", \"explainlikeimfive\", \"askscience\", \"EarthPorn\", \"books\"]\n",
    "\n",
    "decompress()\n",
    "# generateTarget(top20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelNetwork:\n",
    "    def __init__(self, in_size, lstm_size, num_layers, out_size, session,\n",
    "                 learning_rate=0.003, name=\"rnn\"):\n",
    "        self.scope = name\n",
    "        self.in_size = in_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.out_size = out_size\n",
    "        self.session = session\n",
    "        self.learning_rate = tf.constant(learning_rate)\n",
    "        # Last state of LSTM, used when running the network in TEST mode\n",
    "        self.lstm_last_state = np.zeros(\n",
    "            (self.num_layers * 2 * self.lstm_size,)\n",
    "        )\n",
    "        with tf.variable_scope(self.scope):\n",
    "            # (batch_size, timesteps, in_size)\n",
    "            self.xinput = tf.placeholder(\n",
    "                tf.float32,\n",
    "                shape=(None, None, self.in_size),\n",
    "                name=\"xinput\"\n",
    "            )\n",
    "            self.lstm_init_value = tf.placeholder(\n",
    "                tf.float32,\n",
    "                shape=(None, self.num_layers * 2 * self.lstm_size),\n",
    "                name=\"lstm_init_value\"\n",
    "            )\n",
    "            # LSTM\n",
    "            self.lstm_cells = [\n",
    "                tf.contrib.rnn.BasicLSTMCell(\n",
    "                    self.lstm_size,\n",
    "                    forget_bias=1.0,\n",
    "                    state_is_tuple=False\n",
    "                ) for i in range(self.num_layers)\n",
    "            ]\n",
    "            self.lstm = tf.contrib.rnn.MultiRNNCell(\n",
    "                self.lstm_cells,\n",
    "                state_is_tuple=False\n",
    "            )\n",
    "            # Iteratively compute output of recurrent network\n",
    "            outputs, self.lstm_new_state = tf.nn.dynamic_rnn(\n",
    "                self.lstm,\n",
    "                self.xinput,\n",
    "                initial_state=self.lstm_init_value,\n",
    "                dtype=tf.float32\n",
    "            )\n",
    "            # Linear activation (FC layer on top of the LSTM net)\n",
    "            self.rnn_out_W = tf.Variable(\n",
    "                tf.random_normal(\n",
    "                    (self.lstm_size, self.out_size),\n",
    "                    stddev=0.01\n",
    "                )\n",
    "            )\n",
    "            self.rnn_out_B = tf.Variable(\n",
    "                tf.random_normal(\n",
    "                    (self.out_size,), stddev=0.01\n",
    "                )\n",
    "            )\n",
    "            outputs_reshaped = tf.reshape(outputs, [-1, self.lstm_size])\n",
    "            network_output = tf.matmul(\n",
    "                outputs_reshaped,\n",
    "                self.rnn_out_W\n",
    "            ) + self.rnn_out_B\n",
    "            batch_time_shape = tf.shape(outputs)\n",
    "            self.final_outputs = tf.reshape(\n",
    "                tf.nn.softmax(network_output),\n",
    "                (batch_time_shape[0], batch_time_shape[1], self.out_size)\n",
    "            )\n",
    "            # Training: provide target outputs for supervised training.\n",
    "            self.y_batch = tf.placeholder(\n",
    "                tf.float32,\n",
    "                (None, None, self.out_size)\n",
    "            )\n",
    "            y_batch_long = tf.reshape(self.y_batch, [-1, self.out_size])\n",
    "            self.cost = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    logits=network_output,\n",
    "                    labels=y_batch_long\n",
    "                )\n",
    "            )\n",
    "            self.train_op = tf.train.RMSPropOptimizer(\n",
    "                self.learning_rate,\n",
    "                0.9\n",
    "            ).minimize(self.cost)\n",
    "\n",
    "    # Input: X is a single element, not a list!\n",
    "    def run_step(self, x, init_zero_state=True):\n",
    "        # Reset the initial state of the network.\n",
    "        if init_zero_state:\n",
    "            init_value = np.zeros((self.num_layers * 2 * self.lstm_size,))\n",
    "        else:\n",
    "            init_value = self.lstm_last_state\n",
    "        out, next_lstm_state = self.session.run(\n",
    "            [self.final_outputs, self.lstm_new_state],\n",
    "            feed_dict={\n",
    "                self.xinput: [x],\n",
    "                self.lstm_init_value: [init_value]\n",
    "            }\n",
    "        )\n",
    "        self.lstm_last_state = next_lstm_state[0]\n",
    "        return out[0][0]\n",
    "\n",
    "    # xbatch must be (batch_size, timesteps, input_size)\n",
    "    # ybatch must be (batch_size, timesteps, output_size)\n",
    "    def train_batch(self, xbatch, ybatch):\n",
    "        init_value = np.zeros(\n",
    "            (xbatch.shape[0], self.num_layers * 2 * self.lstm_size)\n",
    "        )\n",
    "        cost, _ = self.session.run(\n",
    "            [self.cost, self.train_op],\n",
    "            feed_dict={\n",
    "                self.xinput: xbatch,\n",
    "                self.y_batch: ybatch,\n",
    "                self.lstm_init_value: init_value\n",
    "            }\n",
    "        )\n",
    "        return cost\n",
    "\n",
    "\n",
    "def embed_to_vocab(data_, vocab):\n",
    "    \"\"\"\n",
    "    Embed string to character-arrays -- it generates an array len(data)\n",
    "    x len(vocab).\n",
    "    Vocab is a list of elements.\n",
    "    \"\"\"\n",
    "    data = np.zeros((len(data_), len(vocab)))\n",
    "    cnt = 0\n",
    "    for s in data_:\n",
    "        v = [0.0] * len(vocab)\n",
    "        v[vocab.index(s)] = 1.0\n",
    "        data[cnt, :] = v\n",
    "        cnt += 1\n",
    "    return data\n",
    "\n",
    "\n",
    "def decode_embed(array, vocab):\n",
    "    return vocab[array.index(1)]\n",
    "\n",
    "\n",
    "def load_data(input):\n",
    "    # Load the data\n",
    "    data_ = \"\"\n",
    "    with open(input, 'r') as f:\n",
    "        data_ += f.read()\n",
    "    data_ = data_.lower()\n",
    "    # Convert to 1-hot coding\n",
    "    vocab = sorted(list(set(data_)))\n",
    "    data = embed_to_vocab(data_, vocab)\n",
    "    return data, vocab\n",
    "\n",
    "\n",
    "def check_restore_parameters(sess, saver):\n",
    "    \"\"\" Restore the previously trained parameters if there are any. \"\"\"\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('saved/checkpoint'))\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(input_file):\n",
    "    ckpt_file = \"saved/model.ckpt\"\n",
    "\n",
    "    data, vocab = load_data(input_file)\n",
    "\n",
    "    in_size = out_size = len(vocab)\n",
    "    lstm_size = 256  # 128\n",
    "    num_layers = 2\n",
    "    batch_size = 64  # 128\n",
    "    time_steps = 100  # 50\n",
    "\n",
    "    NUM_TRAIN_BATCHES = 20000\n",
    "\n",
    "    # Number of test characters of text to generate after training the network\n",
    "    LEN_TEST_TEXT = 500\n",
    "\n",
    "    # Initialize the network\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.InteractiveSession(config=config)\n",
    "    net = ModelNetwork(\n",
    "        in_size=in_size,\n",
    "        lstm_size=lstm_size,\n",
    "        num_layers=num_layers,\n",
    "        out_size=out_size,\n",
    "        session=sess,\n",
    "        learning_rate=0.003,\n",
    "        name=\"char_rnn_network\"\n",
    "    )\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    # TRAIN THE NETWORK\n",
    "#     check_restore_parameters(sess, saver)\n",
    "    last_time = time.time()\n",
    "    batch = np.zeros((batch_size, time_steps, in_size))\n",
    "    batch_y = np.zeros((batch_size, time_steps, in_size))\n",
    "    possible_batch_ids = range(data.shape[0] - time_steps - 1)\n",
    "\n",
    "    for i in tnrange(NUM_TRAIN_BATCHES):\n",
    "        # Sample time_steps consecutive samples from the dataset text file\n",
    "        batch_id = random.sample(possible_batch_ids, batch_size)\n",
    "\n",
    "        for j in range(time_steps):\n",
    "            ind1 = [k + j for k in batch_id]\n",
    "            ind2 = [k + j + 1 for k in batch_id]\n",
    "\n",
    "            batch[:, j, :] = data[ind1, :]\n",
    "            batch_y[:, j, :] = data[ind2, :]\n",
    "\n",
    "        cst = net.train_batch(batch, batch_y)\n",
    "\n",
    "        if (i % 100) == 0:\n",
    "            new_time = time.time()\n",
    "            diff = new_time - last_time\n",
    "            last_time = new_time\n",
    "            print(\"batch: {}  loss: {}  speed: {} batches / s\".format(\n",
    "                i, cst, 100 / diff\n",
    "            ))\n",
    "            saver.save(sess, ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(input_file, prefix):\n",
    "    ckpt_file = \"saved/model.ckpt\"\n",
    "\n",
    "    data, vocab = load_data(input_file)\n",
    "\n",
    "    in_size = out_size = len(vocab)\n",
    "    lstm_size = 256  # 128\n",
    "    num_layers = 2\n",
    "    batch_size = 64  # 128\n",
    "    time_steps = 100  # 50\n",
    "\n",
    "    NUM_TRAIN_BATCHES = 20000\n",
    "\n",
    "    # Number of test characters of text to generate after training the network\n",
    "    LEN_TEST_TEXT = 500\n",
    "\n",
    "    # Initialize the network\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.InteractiveSession(config=config)\n",
    "    net = ModelNetwork(\n",
    "        in_size=in_size,\n",
    "        lstm_size=lstm_size,\n",
    "        num_layers=num_layers,\n",
    "        out_size=out_size,\n",
    "        session=sess,\n",
    "        learning_rate=0.003,\n",
    "        name=\"char_rnn_network\"\n",
    "    )\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    # 2) GENERATE LEN_TEST_TEXT CHARACTERS USING THE TRAINED NETWORK\n",
    "    saver.restore(sess, ckpt_file)\n",
    "\n",
    "    TEST_PREFIX = prefix\n",
    "    TEST_PREFIX = TEST_PREFIX.lower()\n",
    "    for i in range(len(TEST_PREFIX)):\n",
    "        out = net.run_step(embed_to_vocab(TEST_PREFIX[i], vocab), i == 0)\n",
    "\n",
    "    print(\"Sentence:\")\n",
    "    gen_str = TEST_PREFIX\n",
    "    for i in range(LEN_TEST_TEXT):\n",
    "        # Sample character from the network according to the generated\n",
    "        # output probabilities.\n",
    "        element = np.random.choice(range(len(vocab)), p=out)\n",
    "        gen_str += vocab[element]\n",
    "        out = net.run_step(embed_to_vocab(vocab[element], vocab), False)\n",
    "\n",
    "    print(gen_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Testing that the model works\n",
    "\n",
    "# train(\"data/shakespeare.txt\")\n",
    "# generate(\"data/shakespeare.txt\", \"The\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def savePickles(startwords, senlens, numsens, filepath):\n",
    "    names = [\"startwords\", \"senlens\", \"numsens\"]\n",
    "    for i in range(len(files)):\n",
    "        with open(filepath + names[i] + '.p', 'wb') as fp:\n",
    "            pickle.dump(files[i], fp)\n",
    "            print (\"Saved {} pickle\".format(names[i]))\n",
    "\n",
    "def loadPickles(filepath):\n",
    "    startwords, senlens, numsens = [], [], []\n",
    "    files = [startwords, senlens, numsens]\n",
    "    names = [\"startwords\", \"senlens\", \"numsens\"]\n",
    "    for i in range(len(names)):\n",
    "        with open(filepath + names[i] + '.p', 'rb') as fp:\n",
    "            files[i] = pickle.load(fp)\n",
    "            print (\"Loaded {} pickle\".format(names[i]))\n",
    "    return startwords, senlens, numsens\n",
    "\n",
    "def writeTXT(inFilename, outFilename):\n",
    "    startwords = set()\n",
    "    senlens = list()\n",
    "    numsens = list()\n",
    "    with open(inFile, \"r\") as inFile:\n",
    "        with open(outFilename, \"w\") as outFile:\n",
    "            for i, line in enumerate(inFile, 1):\n",
    "                comment = json.loads(line)\n",
    "                sentences = nltk.sent_tokenize(comment[\"body\"])\n",
    "                numsens.append(len(sentences))\n",
    "                for sentence in sentences:\n",
    "                    words = nltk.word_tokenize(sentence)\n",
    "                    startwords.add(words[0])\n",
    "                    senlens.append(len(words))\n",
    "                    outFile.write(sentence + \" \")\n",
    "                if i % 1000000 == 0:\n",
    "                    print (\"Processed {} lines\".format(i))\n",
    "    startwords = list(startwords)\n",
    "    return startwords, senlens, numsens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top20 = [\"announcements\", \"funny\", \"AskReddit\", \"todayilearned\", \"science\", \"worldnews\",\n",
    "         \"pics\", \"IAmA\", \"gaming\", \"videos\", \"movies\", \"aww\", \"Music\", \"blog\", \"gifs\",\n",
    "         \"news\", \"explainlikeimfive\", \"askscience\", \"EarthPorn\", \"books\"]\n",
    "features = [\"nc20\",\"nc30\",\"sc1000\", \"ct2000\"]\n",
    "\n",
    "for subreddit in top20:\n",
    "    for feature in features:\n",
    "        filepath = \"data/target{}{}.json\".format(subreddit, feature)\n",
    "        startwords, senlens, numsens = writeTXT(filepath, filepath[:-5]+\"txt\", feature)\n",
    "        savePickles(startwords, senlens, numsens, filepath[:-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# TODO: control comment length. numsens * senlens\n",
    "# TODO: stay ontopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Target r/news, num_child_comments > 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target r/news, num_child_comments > 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target r/funny, num_child_comments > 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target r/funny, num_child_comments > 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target r/funny, score > 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target r/funny, controversy > 2000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
