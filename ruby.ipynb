{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/.conda/envs/cs224/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pprint\n",
    "import argparse\n",
    "import collections\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utils\n",
    "def get_minibatches(data, minibatch_size, shuffle=True):\n",
    "    \"\"\"\n",
    "    Iterates through the provided data one minibatch at at time. You can use this function to\n",
    "    iterate through data in minibatches as follows:\n",
    "\n",
    "        for inputs_minibatch in get_minibatches(inputs, minibatch_size):\n",
    "            ...\n",
    "\n",
    "    Or with multiple data sources:\n",
    "\n",
    "        for inputs_minibatch, labels_minibatch in get_minibatches([inputs, labels], minibatch_size):\n",
    "            ...\n",
    "\n",
    "    Args:\n",
    "        data: there are two possible values:\n",
    "            - a list or numpy array\n",
    "            - a list where each element is either a list or numpy array\n",
    "        minibatch_size: the maximum number of items in a minibatch\n",
    "        shuffle: whether to randomize the order of returned data\n",
    "    Returns:\n",
    "        minibatches: the return value depends on data:\n",
    "            - If data is a list/array it yields the next minibatch of data.\n",
    "            - If data a list of lists/arrays it returns the next minibatch of each element in the\n",
    "              list. This can be used to iterate through multiple data sources\n",
    "              (e.g., features and labels) at the same time.\n",
    "\n",
    "    \"\"\"\n",
    "    list_data = type(data) is list and (type(data[0]) is list or type(data[0]) is np.ndarray)\n",
    "    data_size = len(data[0]) if list_data else len(data)\n",
    "    indices = np.arange(data_size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    for minibatch_start in np.arange(0, data_size, minibatch_size):\n",
    "        minibatch_indices = indices[minibatch_start:minibatch_start + minibatch_size]\n",
    "        yield [minibatch(d, minibatch_indices) for d in data] if list_data \\\n",
    "            else minibatch(data, minibatch_indices)\n",
    "\n",
    "def minibatch(data, minibatch_idx):\n",
    "    return data[minibatch_idx] if type(data) is np.ndarray else [data[i] for i in minibatch_idx]\n",
    "\n",
    "def pad(a, i):\n",
    "    mask = [1] * len(a)\n",
    "    if len(a) > i:\n",
    "        return a[:i], mask[:i]\n",
    "    padding = i - len(a)\n",
    "    return a + [0] * padding, mask + [0] * padding\n",
    "\n",
    "def loadComments(filename, maxComments, config):\n",
    "    comments = []\n",
    "    masks = []\n",
    "    commentps = []\n",
    "    maskps = []\n",
    "    commentfs = []\n",
    "    labels = []\n",
    "    with open(filename, \"r\") as inFile:\n",
    "        for i, line in enumerate(inFile, 1):\n",
    "            if len(comments) >= maxComments:\n",
    "                break\n",
    "            comment = json.loads(line)\n",
    "\n",
    "            commentInput, maskInput = pad(comment[\"body_t\"], config[\"maxDocLength\"])\n",
    "            comments.append(commentInput)\n",
    "            masks.append(maskInput)\n",
    "\n",
    "            commentpInput, maskpInput= pad(comment[\"parent_comment_t\"], config[\"maxDocLength\"])\n",
    "            commentps.append(commentpInput)\n",
    "            maskps.append(maskpInput)\n",
    "\n",
    "            commentf = []\n",
    "            if config[\"addRT\"]:\n",
    "                commentf.append(comment[\"response_time_hours\"])\n",
    "            if config[\"addTime\"]:\n",
    "                commentf.append(comment[\"time_of_day\"])\n",
    "                commentf.append(comment[\"weekday\"])\n",
    "            if config[\"addLength\"]:\n",
    "                commentf.append(len(comment[\"body_t\"]))\n",
    "            commentfs.append(commentf)\n",
    "\n",
    "            if comment[\"num_child_comments\"] == 0:\n",
    "                labels.append([1, 0])\n",
    "            else:\n",
    "                labels.append([0, 1])\n",
    "\n",
    "            if i % 10000 == 0:\n",
    "                print \"Processed {} lines\".format(i)\n",
    "\n",
    "    return [comments, masks, commentps, maskps, commentfs, labels]\n",
    "\n",
    "def printConfig(config):\n",
    "    print \"-----------------------------------------\"\n",
    "    print [\"{}: {}\".format(k, v) for k, v in sorted(config.iteritems())]\n",
    "    print \"-----------------------------------------\"\n",
    "\n",
    "def plot(losses, trainAccuracies, devAccuracies, outputFile=\"plot\"):\n",
    "    xs = range(1, len(losses) + 1)\n",
    "    plt.figure()\n",
    "    plt.plot(xs, losses, \"r-\", label=\"loss\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.savefig(outputFile + \"1.png\")\n",
    "\n",
    "    plt.figure()\n",
    "    trainAcc, = plt.plot(xs, trainAccuracies, \"b-\", label=\"trainAcc\")\n",
    "    devAcc, = plt.plot(xs, devAccuracies, \"g-\", label=\"devAcc\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.legend(handles=[trainAcc, devAcc])\n",
    "    plt.savefig(outputFile + \"2.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preprep\n",
    "\n",
    "# TODO: Uses too much RAM, can't go past 300,000 comments likely, will fix later\n",
    "# TODO: Currently only supports a value of 1 for MIN_FREQ.\n",
    "MIN_FREQ = 1\n",
    "TOKEN_PAD = \"TOKEN_PAD\"\n",
    "TOKEN_UNK = \"TOKEN_UNK\"\n",
    "\n",
    "def process_comment(comment, vocab, frequencies):\n",
    "    processedComment = []\n",
    "    for word in nltk.word_tokenize(comment):\n",
    "        word = process_word(word)\n",
    "        if word not in vocab:\n",
    "            vocab[word] = np.random.randn(len(vocab[TOKEN_PAD]))\n",
    "            frequencies[word] = 0\n",
    "        frequencies[word] += 1\n",
    "        processedComment.append(word)\n",
    "    return processedComment\n",
    "\n",
    "def process_word(word):\n",
    "    if 'http' in word:\n",
    "        return 'TOKEN_HTTP_URL'\n",
    "    if 'ftp' in word:\n",
    "        return 'TOKEN_FTP_URL'\n",
    "    if '@' in word:\n",
    "        return 'TOKEN_AT_REFERENCE'\n",
    "    word = word.lower()\n",
    "    return word\n",
    "\n",
    "def processComments(filename, numLines, vocab, frequencies):\n",
    "    comments = []\n",
    "    with open(filename, \"r\") as inFile:\n",
    "        for i, line in enumerate(inFile, 1):\n",
    "            if len(comments) >= numLines:\n",
    "                break\n",
    "            comment = json.loads(line)\n",
    "            comment[\"body_t\"] = process_comment(comment[\"body\"], vocab, frequencies)\n",
    "            comment[\"parent_comment_t\"] = process_comment(comment[\"parent_comment\"], vocab, frequencies)\n",
    "            comments.append(comment)\n",
    "\n",
    "            if i % 100000 == 0:\n",
    "                print \"Processed {} lines\".format(i)\n",
    "\n",
    "    return comments\n",
    "\n",
    "def cleanFrequencies(vocab, frequencies):\n",
    "    assert len(vocab) - 2 == len(frequencies)\n",
    "\n",
    "    # Take care of special padding token.\n",
    "    embed = [vocab[TOKEN_PAD], vocab[TOKEN_UNK]]\n",
    "    vocab[TOKEN_PAD] = 0\n",
    "    vocab[TOKEN_UNK] = 1\n",
    "\n",
    "    # Loop through all words\n",
    "    for word, count in frequencies.iteritems():\n",
    "        if count < MIN_FREQ:\n",
    "            del vocab[word]\n",
    "            continue\n",
    "        embed.append(vocab[word])\n",
    "        vocab[word] = len(embed) - 1\n",
    "\n",
    "    return vocab, np.asarray(embed)\n",
    "\n",
    "def wordToIndex(word, vocab):\n",
    "    if word in vocab:\n",
    "        return vocab[word]\n",
    "    return vocab[TOKEN_UNK]\n",
    "\n",
    "def outputComments(comments, filename, vocab):\n",
    "    with open(filename, \"w\") as outFile:\n",
    "        for i, comment in enumerate(comments, 1):\n",
    "            comment[\"body_t\"] = [wordToIndex(word, vocab) for word in comment[\"body_t\"]]\n",
    "            comment[\"parent_comment_t\"] = [wordToIndex(word, vocab) for word in comment[\"parent_comment_t\"]]\n",
    "            outFile.write(json.dumps(comment) + \"\\n\")\n",
    "\n",
    "            if i % 10000 == 0:\n",
    "                print \"Outputted {} lines\".format(i)\n",
    "\n",
    "def outputVocab(vocab, filename):\n",
    "    vocabList = [TOKEN_UNK] * len(embed)\n",
    "    for word, index in vocab.iteritems():\n",
    "        vocabList[index] = word\n",
    "    with open(filename, \"w\") as outFile:\n",
    "        for word in vocabList:\n",
    "            outFile.write(word.encode('utf-8') + \"\\n\")\n",
    "\n",
    "# Builds a vocab.\n",
    "def loadWordVectors(inFilename):\n",
    "    print \"Loading word vectors\"\n",
    "    embedSize = 0\n",
    "    vocab = {}\n",
    "    frequencies = {}\n",
    "    with open(inFilename, 'r') as inFile:\n",
    "        for i, line in enumerate(inFile, 1):\n",
    "            row = line.strip().split(' ')\n",
    "            vocab[row[0]] = np.array([float(num) for num in row[1:]])\n",
    "            frequencies[row[0]] = 0\n",
    "            embedSize = len(row) - 1\n",
    "\n",
    "            if i % 100000 == 0:\n",
    "                print \"Processed {} lines\".format(i)\n",
    "    vocab[TOKEN_PAD] = np.zeros(embedSize)\n",
    "    vocab[TOKEN_UNK] = np.random.randn(embedSize)\n",
    "    print \"Loaded {} words\".format(len(vocab))\n",
    "\n",
    "    return vocab, frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LSTM\n",
    "\n",
    "# IMPORTANT: Contains the configurations for the LSTM\n",
    "def getConfig():\n",
    "    config = {\n",
    "        \"maxDocLength\": 250,  # Max is 2191\n",
    "        \"batchSize\": 256,\n",
    "        \"addRT\": True,\n",
    "        \"addTime\": False,\n",
    "        \"addLength\": True,\n",
    "        \"addCommentp\": False\n",
    "    }\n",
    "    config[\"addCommentf\"] = config[\"addRT\"] or config[\"addTime\"] or config[\"addLength\"]\n",
    "    config[\"learningRates\"] = [0.01] * 5 + [0.005] * 5 + [0.003] * 5 + [0.002] * 5 + [0.001] * 5 + [0.0005] * 5 + [0.0003] * 5 + [0.0001] * 5\n",
    "    config[\"lstmUnits\"] = 64\n",
    "    config[\"attentionUnits\"] = 32\n",
    "    config[\"layer2Units\"] = 16\n",
    "    config[\"numClasses\"] = 2\n",
    "    config[\"dropoutKeepProb\"] = 0.9\n",
    "    config[\"numTrain\"] = 1000\n",
    "    config[\"numDev\"] = 200\n",
    "    config[\"numEpochs\"] = len(config[\"learningRates\"])\n",
    "\n",
    "    # Junk.\n",
    "    # config[\"learningRates\"] = [0.01] * 3 + [0.005] * 2 + [0.003] * 5 + [0.002] * 10 + [0.001] * 5 + [0.0005] * 5\n",
    "    # learningRates = [0.01] * 10 + [0.005] * 10 + [0.003] * 10 + [0.002] * 10\n",
    "    # learningRates = [0.01] * 3 + [0.005] * 2 + [0.003] * 5 + [0.002] * 10 + [0.001] * 5 + [0.0005] * 5 + [0.0004] * 5 + [0.0003] * 5 + [0.0002] * 5 + [0.0001] * 5\n",
    "    # learningRates = [0.01] * 10 + [0.005] * 10\n",
    "\n",
    "    return config\n",
    "\n",
    "def attention(inputs, attention_size, time_major=False, return_alphas=False):\n",
    "    if isinstance(inputs, tuple):\n",
    "        # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n",
    "        inputs = tf.concat(inputs, 2)\n",
    "\n",
    "    if time_major:\n",
    "        # (T,B,D) => (B,T,D)\n",
    "        inputs = tf.array_ops.transpose(inputs, [1, 0, 2])\n",
    "\n",
    "    hidden_size = inputs.shape[2].value  # D value - hidden size of the RNN layer\n",
    "\n",
    "    # Trainable parameters\n",
    "    w_omega = tf.Variable(tf.random_normal([hidden_size, attention_size], stddev=0.1))\n",
    "    b_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "    u_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "\n",
    "    with tf.name_scope('v'):\n",
    "        # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n",
    "        #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n",
    "        v = tf.tanh(tf.tensordot(inputs, w_omega, axes=1) + b_omega)\n",
    "\n",
    "    # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n",
    "    vu = tf.tensordot(v, u_omega, axes=1, name='vu')  # (B,T) shape\n",
    "    alphas = tf.nn.softmax(vu, name='alphas')         # (B,T) shape\n",
    "\n",
    "    # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n",
    "    output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)\n",
    "\n",
    "    if not return_alphas:\n",
    "        return output\n",
    "    else:\n",
    "        return output, alphas\n",
    "\n",
    "def getAttentionLSTMOutputs(embeddings, masks, dropoutKeepProb, scope, config):\n",
    "    with tf.name_scope(scope):\n",
    "        # LSTM\n",
    "        seqLengths = tf.reduce_sum(masks, axis=1)\n",
    "        lstmCell = tf.contrib.rnn.BasicLSTMCell(config[\"lstmUnits\"])\n",
    "        lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=dropoutKeepProb)\n",
    "        cellOutputs, _ = tf.nn.dynamic_rnn(lstmCell, embeddings, sequence_length=seqLengths, dtype=tf.float32, scope=scope)\n",
    "\n",
    "        # Attention layer\n",
    "        attentionOutputs = attention(cellOutputs, config[\"attentionUnits\"])\n",
    "\n",
    "        # Dropout layer\n",
    "        dropoutOutputs = tf.nn.dropout(attentionOutputs, dropoutKeepProb)\n",
    "\n",
    "        return dropoutOutputs\n",
    "\n",
    "def getLSTMOutputs(embeddings, masks, dropoutKeepProb, scope, config):\n",
    "    with tf.name_scope(scope):\n",
    "        # LSTM\n",
    "        lstmCell = tf.contrib.rnn.BasicLSTMCell(config[\"lstmUnits\"])\n",
    "        lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=dropoutKeepProb)\n",
    "        cellOutputs, _ = tf.nn.dynamic_rnn(lstmCell, embeddings, dtype=tf.float32, scope=scope)\n",
    "\n",
    "        # Output to pred\n",
    "        cellOutputs = tf.transpose(cellOutputs, [2, 0, 1]) # cells, batches, len\n",
    "        maskedOutputs = tf.reduce_sum(cellOutputs * masks, axis=2) / tf.reduce_sum(masks, axis=1)\n",
    "        lstmOutputs = tf.transpose(maskedOutputs, [1, 0]) # batches, cells\n",
    "\n",
    "    return lstmOutputs\n",
    "\n",
    "def train(embed, trainData, devData, config, trainableE=False, error_analysis=False):\n",
    "    # Create input placeholders\n",
    "    comments = tf.placeholder(tf.int32, [None, config[\"maxDocLength\"]])\n",
    "    masks = tf.placeholder(tf.float32, [None, config[\"maxDocLength\"]])\n",
    "    commentps = tf.placeholder(tf.int32, [None, config[\"maxDocLength\"]])\n",
    "    maskps = tf.placeholder(tf.float32, [None, config[\"maxDocLength\"]])\n",
    "    commentfs = tf.placeholder(tf.float32, [None, config[\"numCommentfs\"]])\n",
    "    labels = tf.placeholder(tf.float32, [None, config[\"numClasses\"]])\n",
    "    dropoutKeepProb = tf.placeholder(tf.float32)\n",
    "    learningRate = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Create embedding tranform.\n",
    "    with tf.variable_scope(\"embedding\", reuse=tf.AUTO_REUSE):\n",
    "        E = tf.get_variable(\"E\", initializer=embed, trainable=trainableE)\n",
    "        embeddings = tf.nn.embedding_lookup(E, comments)\n",
    "        embeddingps = tf.nn.embedding_lookup(E, commentps)\n",
    "\n",
    "    # LSTM\n",
    "    lstmOutputs = None\n",
    "    if config[\"attentionUnits\"]:\n",
    "        lstmOutputs = [getAttentionLSTMOutputs(embeddings, masks, dropoutKeepProb, \"lstm\", config)]\n",
    "    else:\n",
    "        lstmOutputs = [getLSTMOutputs(embeddings, masks, dropoutKeepProb, \"lstm\", config)]\n",
    "    if config[\"addCommentp\"]:\n",
    "        lstmOutputs.append(getLSTMOutputs(embeddingps, maskps, dropoutKeepProb, \"lstmp\", config))\n",
    "    if config[\"addCommentf\"]:\n",
    "        lstmOutputs.append(commentfs)\n",
    "    lstmOutputs = tf.concat(lstmOutputs, axis=1)\n",
    "\n",
    "    # Layer 1 ReLu\n",
    "    W1 = tf.get_variable(\n",
    "        \"W1\",\n",
    "        shape=[config[\"numLSTMOutputs\"], config[\"layer2Units\"]],\n",
    "        initializer=tf.initializers.truncated_normal())\n",
    "    b1 = tf.get_variable(\n",
    "        \"b1\", \n",
    "        shape=[config[\"layer2Units\"]], \n",
    "        initializer=tf.constant_initializer(0.1))\n",
    "    layer1Output = tf.nn.relu(tf.matmul(lstmOutputs, W1) + b1)\n",
    "\n",
    "    # Dropout layer\n",
    "    layer1Droutput = tf.nn.dropout(layer1Output, dropoutKeepProb)\n",
    "\n",
    "    # layer 2 softmax\n",
    "    with tf.name_scope(\"layer2\"):\n",
    "        W2 = tf.get_variable(\n",
    "            \"W2\",\n",
    "            shape=[config[\"layer2Units\"], config[\"numClasses\"]],\n",
    "            initializer=tf.initializers.truncated_normal())\n",
    "        b2 = tf.get_variable(\n",
    "            \"b2\",\n",
    "            shape=[config[\"numClasses\"]],\n",
    "            initializer=tf.constant_initializer(0.1))\n",
    "    prediction = tf.matmul(layer1Droutput, W2) + b2\n",
    "\n",
    "    # Accuracy\n",
    "    correctPred = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "    confusion = tf.confusion_matrix(\n",
    "        labels = tf.argmax(labels, 1),\n",
    "        predictions = tf.argmax(prediction, 1)\n",
    "    )\n",
    "    # Loss and optimizer\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learningRate).minimize(loss)\n",
    "\n",
    "    # Saver\n",
    "    # saver = tf.train.Saver()\n",
    "\n",
    "    # Collect Info.\n",
    "    losses = []\n",
    "    trainAccuracies = []\n",
    "    devAccuracies = []\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # Variable Initialization.\n",
    "        # if saveIn:\n",
    "        #     saver.restore(sess, saveIn)\n",
    "        # else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch in range(config[\"numEpochs\"]):\n",
    "            # Training.\n",
    "            epochLoss = 0\n",
    "            epochAccuracy = 0\n",
    "            for batchNum, batches in enumerate(get_minibatches(trainData, config[\"batchSize\"])):\n",
    "                feedDict = {\n",
    "                    comments: batches[0],\n",
    "                    masks: batches[1],\n",
    "                    labels: batches[5],\n",
    "                    learningRate: config[\"learningRates\"][epoch],\n",
    "                    dropoutKeepProb: config[\"dropoutKeepProb\"]\n",
    "                }\n",
    "                if config[\"addCommentp\"]:\n",
    "                    feedDict[commentps] = batches[2]\n",
    "                    feedDict[maskps] = batches[3]\n",
    "                if config[\"addCommentf\"]:\n",
    "                    feedDict[commentfs] = batches[4]\n",
    "\n",
    "                batchSize = len(batches[0])\n",
    "                batchAccuracy, batchLoss, _ = sess.run([accuracy, loss, optimizer], feedDict)\n",
    "                epochLoss += batchLoss * batchSize\n",
    "                epochAccuracy += batchAccuracy * batchSize\n",
    "                if (batchNum + 1) % 100 == 0:\n",
    "                    print \"Epoch: {}, Batch: {}\".format(epoch + 1, batchNum + 1)\n",
    "            losses.append(epochLoss / float(config[\"numTrain\"]))\n",
    "            trainAccuracies.append(epochAccuracy / float(config[\"numTrain\"]))\n",
    "            print \"Epoch: {}, Loss: {}, Accuracy: {}\".format(epoch + 1, losses[-1], trainAccuracies[-1])\n",
    "\n",
    "            # Dev.\n",
    "            epochAccuracy = 0\n",
    "            for batchNum, batches in enumerate(get_minibatches(devData, config[\"batchSize\"])):\n",
    "                feedDict = {\n",
    "                    comments: batches[0],\n",
    "                    masks: batches[1],\n",
    "                    labels: batches[5],\n",
    "                    learningRate: config[\"learningRates\"][epoch],\n",
    "                    dropoutKeepProb: 1.0\n",
    "                }\n",
    "                if config[\"addCommentp\"]:\n",
    "                    feedDict[commentps] = batches[2]\n",
    "                    feedDict[maskps] = batches[3]\n",
    "                if config[\"addCommentf\"]:\n",
    "                    feedDict[commentfs] = batches[4]\n",
    "\n",
    "                batchSize = len(batches[0])\n",
    "                epochAccuracy += sess.run(accuracy, feedDict) * batchSize\n",
    "            devAccuracies.append(epochAccuracy / float(config[\"numDev\"]))\n",
    "            print \"Dev Accuracy: {}\".format(devAccuracies[-1])\n",
    "        print('Confusion Matrix: \\n\\n', tf.Tensor.eval(confusion,feed_dict=None, session=None))\n",
    "\n",
    "            # savePath = saver.save(sess, saveOut)\n",
    "            # print \"Model saved at {}\".format(savePath)\n",
    "\n",
    "    # Print out summary.\n",
    "    bestDevAccuracy = 0\n",
    "    bestIndex = 0\n",
    "    for i, accuracy in enumerate(devAccuracies):\n",
    "        if accuracy > bestDevAccuracy:\n",
    "            bestDevAccuracy = accuracy\n",
    "            bestIndex = i\n",
    "            bestConfusion = confusion\n",
    "\n",
    "    print \"Best Dev of {} at epoch {}, train acc: {}, train loss: {}\".format(\n",
    "        bestDevAccuracy,\n",
    "        bestIndex + 1,\n",
    "        trainAccuracies[bestIndex],\n",
    "        losses[bestIndex])\n",
    "\n",
    "    # Return series.\n",
    "    return losses, trainAccuracies, devAccuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# numLines = float('inf')\n",
    "\n",
    "# vocab, frequencies = loadWordVectors(\"glove.6B.300d.txt\")\n",
    "\n",
    "# print \"Processing Training Data\"\n",
    "# trainComments = processComments(\n",
    "#     \"Reddit2ndTrainTime\",\n",
    "#     numLines,\n",
    "#     vocab,\n",
    "#     frequencies)\n",
    "\n",
    "# print \"Processing Dev Data\"\n",
    "# devComments = processComments(\n",
    "#     \"Reddit2ndDevTime\",\n",
    "#     numLines,\n",
    "#     vocab,\n",
    "#     frequencies)\n",
    "\n",
    "# print \"Cleaning frequencies\"\n",
    "# vocab, embed = cleanFrequencies(vocab, frequencies)\n",
    "# assert len(vocab) == len(embed)\n",
    "# print \"Vocab size: {}\".format(len(vocab))\n",
    "\n",
    "# print \"Outputting train comments\"\n",
    "# outputComments(trainComments, \"data/ProcessedTrain\", vocab)\n",
    "\n",
    "# print \"Outputting dev comments\"\n",
    "# outputComments(devComments, \"data/ProcessedDev\", vocab)\n",
    "\n",
    "# print \"Outputting embeddings\"\n",
    "# np.savetxt(\"data/embed.txt\", embed)\n",
    "\n",
    "# print \"Outputting vocab\"\n",
    "# outputVocab(vocab, \"data/vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config\n",
      "Loading embeddings\n"
     ]
    }
   ],
   "source": [
    "print \"Loading config\"\n",
    "config = getConfig()\n",
    "\n",
    "print \"Loading embeddings\"\n",
    "embed = np.loadtxt(\"data/embed.txt\", dtype=np.float32)\n",
    "print embed.shape\n",
    "\n",
    "print \"Loading Training Data\"\n",
    "trainData = loadComments(\"data/ProcessedTrain\", config[\"numTrain\"], config)\n",
    "\n",
    "print \"Loading Dev Data\"\n",
    "devData = loadComments(\"data/ProcessedDev\", config[\"numDev\"], config)\n",
    "\n",
    "# Additional configs\n",
    "config[\"vocabSize\"] = len(embed)\n",
    "config[\"embedDim\"] = len(embed[0])\n",
    "config[\"numCommentfs\"] = len(trainData[4][0])\n",
    "config[\"numLSTMOutputs\"] = config[\"lstmUnits\"] + config[\"numCommentfs\"]\n",
    "if config[\"addCommentp\"]:\n",
    "    config[\"numLSTMOutputs\"] += config[\"lstmUnits\"]\n",
    "printConfig(config)\n",
    "# config[\"addTime\"] = True\n",
    "# config[\"addCommentp\"] = True\n",
    "\n",
    "print \"Training\"\n",
    "losses, trainAccuracies, devAccuracies = train(\n",
    "    embed, \n",
    "    trainData, \n",
    "    devData, \n",
    "    config,\n",
    "    trainableE=False\n",
    "    )\n",
    "\n",
    "print \"Plotting\"\n",
    "plot(losses, trainAccuracies, devAccuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "def load_summary(filename):\n",
    "    corrects = []\n",
    "    incorrects = []\n",
    "    with open(filename, \"r\") as inFile:\n",
    "        for i, line in enumerate(inFile, 1):\n",
    "            comment = json.loads(line)\n",
    "            correct = 0\n",
    "            if (comment[\"prediction\"] == 1 and comment[\"num_child_comments\"] > 0) or (comment[\"prediction\"] == 0 and comment[\"num_child_comments\"] == 0):\n",
    "                correct = 1\n",
    "            if correct == 1:\n",
    "                corrects.append(comment)\n",
    "            else:\n",
    "                incorrects.append(comment)\n",
    "            if i % 10000 == 0: print(\"Processed {} lines.\".format(i))\n",
    "    return corrects, incorrects\n",
    "# corrects, incorrects = load_summary(\"lstmSummaryPred.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def check_correlation(corrects, incorrects, feature):\n",
    "    X1 = []\n",
    "    y1 = []\n",
    "    X0 = []\n",
    "    y0 = []\n",
    "    if feature == \"length\":\n",
    "        for x in corrects:\n",
    "            y1.append(1)\n",
    "            X1.append(len(x[\"body\"].split()))\n",
    "        for x in incorrects:\n",
    "            y0.append(0)\n",
    "            X0.append(len(x[\"body\"].split()))\n",
    "    elif feature == \"positivechildren\":\n",
    "        for x in corrects:\n",
    "            if x['prediction'] > 0:\n",
    "                y1.append(1)\n",
    "                X1.append(x[\"num_child_comments\"])\n",
    "        for x in incorrects:\n",
    "            if x['prediction'] > 0:\n",
    "                y0.append(0)\n",
    "                X0.append(x[\"num_child_comments\"])\n",
    "    elif feature == \"negativechildren\":\n",
    "        for x in corrects:\n",
    "            if x['prediction'] < 1:\n",
    "                y1.append(1)\n",
    "                X1.append(x[\"num_child_comments\"])\n",
    "        for x in incorrects:\n",
    "            if x['prediction'] < 1:\n",
    "                y0.append(0)\n",
    "                X0.append(x[\"num_child_comments\"])\n",
    "    else:\n",
    "        for x in corrects:\n",
    "            y1.append(1)\n",
    "            X1.append(x[feature])\n",
    "        for x in incorrects:\n",
    "            y0.append(0)\n",
    "            X0.append(x[feature])\n",
    "    X = np.concatenate((X1,X0))\n",
    "    y = np.concatenate((y1,y0))\n",
    "    p = pearsonr(list(X), list(y))\n",
    "    print(\"Pearson Correlation Coefficient: {}\".format(p))\n",
    "        \n",
    "    print(\"Correct Mean: {}, Incorrect Mean: {}\".format(np.mean(X1), np.mean(X0)))\n",
    "    print(\"Correct Median: {}, Incorrect Median: {}\".format(np.median(X1), np.median(X0)))\n",
    "    print(\"Correct Std: {}, Incorrect Std: {}\".format(np.std(X1), np.std(X0)))\n",
    "\n",
    "    xlabel = feature\n",
    "    \n",
    "#     plt.hist(X1)\n",
    "#     plt.xlabel(xlabel)\n",
    "#     plt.ylabel(\"Occurrence\")\n",
    "#     plt.title(\"{} of Corrects\".format(feature))\n",
    "#     plt.grid(True)\n",
    "#     plt.show()\n",
    "    \n",
    "#     plt.hist(X0)\n",
    "#     plt.xlabel(xlabel)\n",
    "#     plt.ylabel(\"Occurrence\")\n",
    "#     plt.title(\"{} of Incorrects\".format(feature))\n",
    "#     plt.grid(True)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlation Coefficient: (0.01612930004926319, 0.0012554694164572599)\n",
      "Correct Mean: 40.66786376274328, Incorrect Mean: 38.798851389676685\n",
      "Correct Median: 24.0, Incorrect Median: 22.0\n",
      "Correct Std: 56.22969145496679, Incorrect Std: 53.715951595926924\n"
     ]
    }
   ],
   "source": [
    "check_correlation(corrects, incorrects, \"length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlation Coefficient: (0.003991702049542469, 0.7778005010572098)\n",
      "Correct Mean: 2.71085080148, Incorrect Mean: 2.6930523918\n",
      "Correct Median: 2.0, Incorrect Median: 2.0\n",
      "Correct Std: 2.13744564268, Incorrect Std: 2.11157392517\n"
     ]
    }
   ],
   "source": [
    "check_correlation(corrects, incorrects, \"weekday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlation Coefficient: (0.00589288468065817, 0.6769788340939595)\n",
      "Correct Mean: 2.81257706535, Incorrect Mean: 2.78986332574\n",
      "Correct Median: 3.0, Incorrect Median: 3.0\n",
      "Correct Std: 1.83556846993, Incorrect Std: 1.84777455988\n"
     ]
    }
   ],
   "source": [
    "check_correlation(corrects, incorrects, \"time_of_day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlation Coefficient: (0.31739853367414167, 4.9383306196631316e-79)\n",
      "Correct Mean: 5.4781420765, Incorrect Mean: 0.0\n",
      "Correct Median: 3.0, Incorrect Median: 0.0\n",
      "Correct Std: 9.57602954796, Incorrect Std: 0.0\n"
     ]
    }
   ],
   "source": [
    "check_correlation(corrects, incorrects, \"positivechildren\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlation Coefficient: (-0.32617383457390564, 1.7941953668691163e-42)\n",
      "Correct Mean: 0.0, Incorrect Mean: 3.79934747145\n",
      "Correct Median: 0.0, Incorrect Median: 2.0\n",
      "Correct Std: 0.0, Incorrect Std: 8.74641103175\n"
     ]
    }
   ],
   "source": [
    "check_correlation(corrects, incorrects, \"negativechildren\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000 lines.\n",
      "Processed 20000 lines.\n",
      "Processed 30000 lines.\n",
      "Processed 40000 lines.\n",
      "Length\n",
      "Pearson Correlation Coefficient: (0.01612930004926319, 0.0012554694164572599)\n",
      "Correct Mean: 40.66786376274328, Incorrect Mean: 38.798851389676685\n",
      "Correct Median: 24.0, Incorrect Median: 22.0\n",
      "Correct Std: 56.22969145496679, Incorrect Std: 53.715951595926924\n",
      "Weekday\n",
      "Pearson Correlation Coefficient: (0.011780517419367403, 0.018467368442818283)\n",
      "Correct Mean: 2.891025641025641, Incorrect Mean: 2.8428814520703347\n",
      "Correct Median: 3.0, Incorrect Median: 3.0\n",
      "Correct Std: 1.9519885454845136, Incorrect Std: 1.9532581333123413\n",
      "Time of Day\n",
      "Pearson Correlation Coefficient: (0.0039594889544921344, 0.42843289629301007)\n",
      "Correct Mean: 2.8466944701884462, Incorrect Mean: 2.831678956324447\n",
      "Correct Median: 3.0, Incorrect Median: 3.0\n",
      "Correct Std: 1.8142139616597575, Incorrect Std: 1.807536176113114\n",
      "True positives and False positives\n",
      "Pearson Correlation Coefficient: (0.29282870291727275, 0.0)\n",
      "Correct Mean: 5.5920670081683514, Incorrect Mean: 0.0\n",
      "Correct Median: 3.0, Incorrect Median: 0.0\n",
      "Correct Std: 10.507478566910386, Incorrect Std: 0.0\n",
      "True negatives and False negatives\n",
      "Pearson Correlation Coefficient: (-0.28821688365920001, 0.0)\n",
      "Correct Mean: 0.0, Incorrect Mean: 3.97885500575374\n",
      "Correct Median: 0.0, Incorrect Median: 2.0\n",
      "Correct Std: 0.0, Incorrect Std: 10.427429441858504\n"
     ]
    }
   ],
   "source": [
    "corrects, incorrects = load_summary(\"lstmSummaryPredT34.json\")\n",
    "print(\"Length\")\n",
    "check_correlation(corrects, incorrects, \"length\")\n",
    "print(\"Weekday\")\n",
    "check_correlation(corrects, incorrects, \"weekday\")\n",
    "print(\"Time of Day\")\n",
    "check_correlation(corrects, incorrects, \"time_of_day\")\n",
    "print(\"True positives and False positives\")\n",
    "check_correlation(corrects, incorrects, \"positivechildren\")\n",
    "print(\"True negatives and False negatives\")\n",
    "check_correlation(corrects, incorrects, \"negativechildren\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
