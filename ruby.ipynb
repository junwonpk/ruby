{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/.conda/envs/cs224/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pprint\n",
    "import argparse\n",
    "import collections\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utils\n",
    "def get_minibatches(data, minibatch_size, shuffle=True):\n",
    "    \"\"\"\n",
    "    Iterates through the provided data one minibatch at at time. You can use this function to\n",
    "    iterate through data in minibatches as follows:\n",
    "\n",
    "        for inputs_minibatch in get_minibatches(inputs, minibatch_size):\n",
    "            ...\n",
    "\n",
    "    Or with multiple data sources:\n",
    "\n",
    "        for inputs_minibatch, labels_minibatch in get_minibatches([inputs, labels], minibatch_size):\n",
    "            ...\n",
    "\n",
    "    Args:\n",
    "        data: there are two possible values:\n",
    "            - a list or numpy array\n",
    "            - a list where each element is either a list or numpy array\n",
    "        minibatch_size: the maximum number of items in a minibatch\n",
    "        shuffle: whether to randomize the order of returned data\n",
    "    Returns:\n",
    "        minibatches: the return value depends on data:\n",
    "            - If data is a list/array it yields the next minibatch of data.\n",
    "            - If data a list of lists/arrays it returns the next minibatch of each element in the\n",
    "              list. This can be used to iterate through multiple data sources\n",
    "              (e.g., features and labels) at the same time.\n",
    "\n",
    "    \"\"\"\n",
    "    list_data = type(data) is list and (type(data[0]) is list or type(data[0]) is np.ndarray)\n",
    "    data_size = len(data[0]) if list_data else len(data)\n",
    "    indices = np.arange(data_size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    for minibatch_start in np.arange(0, data_size, minibatch_size):\n",
    "        minibatch_indices = indices[minibatch_start:minibatch_start + minibatch_size]\n",
    "        yield [minibatch(d, minibatch_indices) for d in data] if list_data \\\n",
    "            else minibatch(data, minibatch_indices)\n",
    "\n",
    "def minibatch(data, minibatch_idx):\n",
    "    return data[minibatch_idx] if type(data) is np.ndarray else [data[i] for i in minibatch_idx]\n",
    "\n",
    "def pad(a, i):\n",
    "    mask = [1] * len(a)\n",
    "    if len(a) > i:\n",
    "        return a[:i], mask[:i]\n",
    "    padding = i - len(a)\n",
    "    return a + [0] * padding, mask + [0] * padding\n",
    "\n",
    "def loadComments(filename, maxComments, config):\n",
    "    comments = []\n",
    "    masks = []\n",
    "    commentps = []\n",
    "    maskps = []\n",
    "    commentfs = []\n",
    "    labels = []\n",
    "    with open(filename, \"r\") as inFile:\n",
    "        for i, line in enumerate(inFile, 1):\n",
    "            if len(comments) >= maxComments:\n",
    "                break\n",
    "            comment = json.loads(line)\n",
    "\n",
    "            commentInput, maskInput = pad(comment[\"body_t\"], config[\"maxDocLength\"])\n",
    "            comments.append(commentInput)\n",
    "            masks.append(maskInput)\n",
    "\n",
    "            commentpInput, maskpInput= pad(comment[\"parent_comment_t\"], config[\"maxDocLength\"])\n",
    "            commentps.append(commentpInput)\n",
    "            maskps.append(maskpInput)\n",
    "\n",
    "            commentf = []\n",
    "            if config[\"addRT\"]:\n",
    "                commentf.append(comment[\"response_time_hours\"])\n",
    "            if config[\"addTime\"]:\n",
    "                commentf.append(comment[\"time_of_day\"])\n",
    "                commentf.append(comment[\"weekday\"])\n",
    "            if config[\"addLength\"]:\n",
    "                commentf.append(len(comment[\"body_t\"]))\n",
    "            commentfs.append(commentf)\n",
    "\n",
    "            if comment[\"num_child_comments\"] == 0:\n",
    "                labels.append([1, 0])\n",
    "            else:\n",
    "                labels.append([0, 1])\n",
    "\n",
    "            if i % 10000 == 0:\n",
    "                print \"Processed {} lines\".format(i)\n",
    "\n",
    "    return [comments, masks, commentps, maskps, commentfs, labels]\n",
    "\n",
    "def printConfig(config):\n",
    "    print \"-----------------------------------------\"\n",
    "    print [\"{}: {}\".format(k, v) for k, v in sorted(config.iteritems())]\n",
    "    print \"-----------------------------------------\"\n",
    "\n",
    "def plot(losses, trainAccuracies, devAccuracies, outputFile=\"plot\"):\n",
    "    xs = range(1, len(losses) + 1)\n",
    "    plt.figure()\n",
    "    plt.plot(xs, losses, \"r-\", label=\"loss\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.savefig(outputFile + \"1.png\")\n",
    "\n",
    "    plt.figure()\n",
    "    trainAcc, = plt.plot(xs, trainAccuracies, \"b-\", label=\"trainAcc\")\n",
    "    devAcc, = plt.plot(xs, devAccuracies, \"g-\", label=\"devAcc\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.legend(handles=[trainAcc, devAcc])\n",
    "    plt.savefig(outputFile + \"2.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preprep\n",
    "\n",
    "# TODO: Uses too much RAM, can't go past 300,000 comments likely, will fix later\n",
    "# TODO: Currently only supports a value of 1 for MIN_FREQ.\n",
    "MIN_FREQ = 1\n",
    "TOKEN_PAD = \"TOKEN_PAD\"\n",
    "TOKEN_UNK = \"TOKEN_UNK\"\n",
    "\n",
    "def process_comment(comment, vocab, frequencies):\n",
    "    processedComment = []\n",
    "    for word in nltk.word_tokenize(comment):\n",
    "        word = process_word(word)\n",
    "        if word not in vocab:\n",
    "            vocab[word] = np.random.randn(len(vocab[TOKEN_PAD]))\n",
    "            frequencies[word] = 0\n",
    "        frequencies[word] += 1\n",
    "        processedComment.append(word)\n",
    "    return processedComment\n",
    "\n",
    "def process_word(word):\n",
    "    if 'http' in word:\n",
    "        return 'TOKEN_HTTP_URL'\n",
    "    if 'ftp' in word:\n",
    "        return 'TOKEN_FTP_URL'\n",
    "    if '@' in word:\n",
    "        return 'TOKEN_AT_REFERENCE'\n",
    "    word = word.lower()\n",
    "    return word\n",
    "\n",
    "def processComments(filename, numLines, vocab, frequencies):\n",
    "    comments = []\n",
    "    with open(filename, \"r\") as inFile:\n",
    "        for i, line in enumerate(inFile, 1):\n",
    "            if len(comments) >= numLines:\n",
    "                break\n",
    "            comment = json.loads(line)\n",
    "            comment[\"body_t\"] = process_comment(comment[\"body\"], vocab, frequencies)\n",
    "            comment[\"parent_comment_t\"] = process_comment(comment[\"parent_comment\"], vocab, frequencies)\n",
    "            comments.append(comment)\n",
    "\n",
    "            if i % 100000 == 0:\n",
    "                print \"Processed {} lines\".format(i)\n",
    "\n",
    "    return comments\n",
    "\n",
    "def cleanFrequencies(vocab, frequencies):\n",
    "    assert len(vocab) - 2 == len(frequencies)\n",
    "\n",
    "    # Take care of special padding token.\n",
    "    embed = [vocab[TOKEN_PAD], vocab[TOKEN_UNK]]\n",
    "    vocab[TOKEN_PAD] = 0\n",
    "    vocab[TOKEN_UNK] = 1\n",
    "\n",
    "    # Loop through all words\n",
    "    for word, count in frequencies.iteritems():\n",
    "        if count < MIN_FREQ:\n",
    "            del vocab[word]\n",
    "            continue\n",
    "        embed.append(vocab[word])\n",
    "        vocab[word] = len(embed) - 1\n",
    "\n",
    "    return vocab, np.asarray(embed)\n",
    "\n",
    "def wordToIndex(word, vocab):\n",
    "    if word in vocab:\n",
    "        return vocab[word]\n",
    "    return vocab[TOKEN_UNK]\n",
    "\n",
    "def outputComments(comments, filename, vocab):\n",
    "    with open(filename, \"w\") as outFile:\n",
    "        for i, comment in enumerate(comments, 1):\n",
    "            comment[\"body_t\"] = [wordToIndex(word, vocab) for word in comment[\"body_t\"]]\n",
    "            comment[\"parent_comment_t\"] = [wordToIndex(word, vocab) for word in comment[\"parent_comment_t\"]]\n",
    "            outFile.write(json.dumps(comment) + \"\\n\")\n",
    "\n",
    "            if i % 10000 == 0:\n",
    "                print \"Outputted {} lines\".format(i)\n",
    "\n",
    "def outputVocab(vocab, filename):\n",
    "    vocabList = [TOKEN_UNK] * len(embed)\n",
    "    for word, index in vocab.iteritems():\n",
    "        vocabList[index] = word\n",
    "    with open(filename, \"w\") as outFile:\n",
    "        for word in vocabList:\n",
    "            outFile.write(word.encode('utf-8') + \"\\n\")\n",
    "\n",
    "# Builds a vocab.\n",
    "def loadWordVectors(inFilename):\n",
    "    print \"Loading word vectors\"\n",
    "    embedSize = 0\n",
    "    vocab = {}\n",
    "    frequencies = {}\n",
    "    with open(inFilename, 'r') as inFile:\n",
    "        for i, line in enumerate(inFile, 1):\n",
    "            row = line.strip().split(' ')\n",
    "            vocab[row[0]] = np.array([float(num) for num in row[1:]])\n",
    "            frequencies[row[0]] = 0\n",
    "            embedSize = len(row) - 1\n",
    "\n",
    "            if i % 100000 == 0:\n",
    "                print \"Processed {} lines\".format(i)\n",
    "    vocab[TOKEN_PAD] = np.zeros(embedSize)\n",
    "    vocab[TOKEN_UNK] = np.random.randn(embedSize)\n",
    "    print \"Loaded {} words\".format(len(vocab))\n",
    "\n",
    "    return vocab, frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LSTM\n",
    "\n",
    "# IMPORTANT: Contains the configurations for the LSTM\n",
    "def getConfig():\n",
    "    config = {\n",
    "        \"maxDocLength\": 250,  # Max is 2191\n",
    "        \"batchSize\": 256,\n",
    "        \"addRT\": True,\n",
    "        \"addTime\": False,\n",
    "        \"addLength\": True,\n",
    "        \"addCommentp\": False\n",
    "    }\n",
    "    config[\"addCommentf\"] = config[\"addRT\"] or config[\"addTime\"] or config[\"addLength\"]\n",
    "    config[\"learningRates\"] = [0.01] * 5 + [0.005] * 5 + [0.003] * 5 + [0.002] * 5 + [0.001] * 5 + [0.0005] * 5 + [0.0003] * 5 + [0.0001] * 5\n",
    "    config[\"lstmUnits\"] = 64\n",
    "    config[\"attentionUnits\"] = 32\n",
    "    config[\"layer2Units\"] = 16\n",
    "    config[\"numClasses\"] = 2\n",
    "    config[\"dropoutKeepProb\"] = 0.9\n",
    "    config[\"numTrain\"] = 1000\n",
    "    config[\"numDev\"] = 200\n",
    "    config[\"numEpochs\"] = len(config[\"learningRates\"])\n",
    "\n",
    "    # Junk.\n",
    "    # config[\"learningRates\"] = [0.01] * 3 + [0.005] * 2 + [0.003] * 5 + [0.002] * 10 + [0.001] * 5 + [0.0005] * 5\n",
    "    # learningRates = [0.01] * 10 + [0.005] * 10 + [0.003] * 10 + [0.002] * 10\n",
    "    # learningRates = [0.01] * 3 + [0.005] * 2 + [0.003] * 5 + [0.002] * 10 + [0.001] * 5 + [0.0005] * 5 + [0.0004] * 5 + [0.0003] * 5 + [0.0002] * 5 + [0.0001] * 5\n",
    "    # learningRates = [0.01] * 10 + [0.005] * 10\n",
    "\n",
    "    return config\n",
    "\n",
    "def attention(inputs, attention_size, time_major=False, return_alphas=False):\n",
    "    if isinstance(inputs, tuple):\n",
    "        # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n",
    "        inputs = tf.concat(inputs, 2)\n",
    "\n",
    "    if time_major:\n",
    "        # (T,B,D) => (B,T,D)\n",
    "        inputs = tf.array_ops.transpose(inputs, [1, 0, 2])\n",
    "\n",
    "    hidden_size = inputs.shape[2].value  # D value - hidden size of the RNN layer\n",
    "\n",
    "    # Trainable parameters\n",
    "    w_omega = tf.Variable(tf.random_normal([hidden_size, attention_size], stddev=0.1))\n",
    "    b_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "    u_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "\n",
    "    with tf.name_scope('v'):\n",
    "        # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n",
    "        #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n",
    "        v = tf.tanh(tf.tensordot(inputs, w_omega, axes=1) + b_omega)\n",
    "\n",
    "    # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n",
    "    vu = tf.tensordot(v, u_omega, axes=1, name='vu')  # (B,T) shape\n",
    "    alphas = tf.nn.softmax(vu, name='alphas')         # (B,T) shape\n",
    "\n",
    "    # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n",
    "    output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)\n",
    "\n",
    "    if not return_alphas:\n",
    "        return output\n",
    "    else:\n",
    "        return output, alphas\n",
    "\n",
    "def getAttentionLSTMOutputs(embeddings, masks, dropoutKeepProb, scope, config):\n",
    "    with tf.name_scope(scope):\n",
    "        # LSTM\n",
    "        seqLengths = tf.reduce_sum(masks, axis=1)\n",
    "        lstmCell = tf.contrib.rnn.BasicLSTMCell(config[\"lstmUnits\"])\n",
    "        lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=dropoutKeepProb)\n",
    "        cellOutputs, _ = tf.nn.dynamic_rnn(lstmCell, embeddings, sequence_length=seqLengths, dtype=tf.float32, scope=scope)\n",
    "\n",
    "        # Attention layer\n",
    "        attentionOutputs = attention(cellOutputs, config[\"attentionUnits\"])\n",
    "\n",
    "        # Dropout layer\n",
    "        dropoutOutputs = tf.nn.dropout(attentionOutputs, dropoutKeepProb)\n",
    "\n",
    "        return dropoutOutputs\n",
    "\n",
    "def getLSTMOutputs(embeddings, masks, dropoutKeepProb, scope, config):\n",
    "    with tf.name_scope(scope):\n",
    "        # LSTM\n",
    "        lstmCell = tf.contrib.rnn.BasicLSTMCell(config[\"lstmUnits\"])\n",
    "        lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=dropoutKeepProb)\n",
    "        cellOutputs, _ = tf.nn.dynamic_rnn(lstmCell, embeddings, dtype=tf.float32, scope=scope)\n",
    "\n",
    "        # Output to pred\n",
    "        cellOutputs = tf.transpose(cellOutputs, [2, 0, 1]) # cells, batches, len\n",
    "        maskedOutputs = tf.reduce_sum(cellOutputs * masks, axis=2) / tf.reduce_sum(masks, axis=1)\n",
    "        lstmOutputs = tf.transpose(maskedOutputs, [1, 0]) # batches, cells\n",
    "\n",
    "    return lstmOutputs\n",
    "\n",
    "def train(embed, trainData, devData, config, trainableE=False, error_analysis=False):\n",
    "    # Create input placeholders\n",
    "    comments = tf.placeholder(tf.int32, [None, config[\"maxDocLength\"]])\n",
    "    masks = tf.placeholder(tf.float32, [None, config[\"maxDocLength\"]])\n",
    "    commentps = tf.placeholder(tf.int32, [None, config[\"maxDocLength\"]])\n",
    "    maskps = tf.placeholder(tf.float32, [None, config[\"maxDocLength\"]])\n",
    "    commentfs = tf.placeholder(tf.float32, [None, config[\"numCommentfs\"]])\n",
    "    labels = tf.placeholder(tf.float32, [None, config[\"numClasses\"]])\n",
    "    dropoutKeepProb = tf.placeholder(tf.float32)\n",
    "    learningRate = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Create embedding tranform.\n",
    "    with tf.variable_scope(\"embedding\", reuse=tf.AUTO_REUSE):\n",
    "        E = tf.get_variable(\"E\", initializer=embed, trainable=trainableE)\n",
    "        embeddings = tf.nn.embedding_lookup(E, comments)\n",
    "        embeddingps = tf.nn.embedding_lookup(E, commentps)\n",
    "\n",
    "    # LSTM\n",
    "    lstmOutputs = None\n",
    "    if config[\"attentionUnits\"]:\n",
    "        lstmOutputs = [getAttentionLSTMOutputs(embeddings, masks, dropoutKeepProb, \"lstm\", config)]\n",
    "    else:\n",
    "        lstmOutputs = [getLSTMOutputs(embeddings, masks, dropoutKeepProb, \"lstm\", config)]\n",
    "    if config[\"addCommentp\"]:\n",
    "        lstmOutputs.append(getLSTMOutputs(embeddingps, maskps, dropoutKeepProb, \"lstmp\", config))\n",
    "    if config[\"addCommentf\"]:\n",
    "        lstmOutputs.append(commentfs)\n",
    "    lstmOutputs = tf.concat(lstmOutputs, axis=1)\n",
    "\n",
    "    # Layer 1 ReLu\n",
    "    W1 = tf.get_variable(\n",
    "        \"W1\",\n",
    "        shape=[config[\"numLSTMOutputs\"], config[\"layer2Units\"]],\n",
    "        initializer=tf.initializers.truncated_normal())\n",
    "    b1 = tf.get_variable(\n",
    "        \"b1\", \n",
    "        shape=[config[\"layer2Units\"]], \n",
    "        initializer=tf.constant_initializer(0.1))\n",
    "    layer1Output = tf.nn.relu(tf.matmul(lstmOutputs, W1) + b1)\n",
    "\n",
    "    # Dropout layer\n",
    "    layer1Droutput = tf.nn.dropout(layer1Output, dropoutKeepProb)\n",
    "\n",
    "    # layer 2 softmax\n",
    "    with tf.name_scope(\"layer2\"):\n",
    "        W2 = tf.get_variable(\n",
    "            \"W2\",\n",
    "            shape=[config[\"layer2Units\"], config[\"numClasses\"]],\n",
    "            initializer=tf.initializers.truncated_normal())\n",
    "        b2 = tf.get_variable(\n",
    "            \"b2\",\n",
    "            shape=[config[\"numClasses\"]],\n",
    "            initializer=tf.constant_initializer(0.1))\n",
    "    prediction = tf.matmul(layer1Droutput, W2) + b2\n",
    "\n",
    "    # Accuracy\n",
    "    correctPred = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "    confusion = tf.confusion_matrix(\n",
    "        labels = tf.argmax(labels, 1),\n",
    "        predictions = tf.argmax(prediction, 1)\n",
    "    )\n",
    "    # Loss and optimizer\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learningRate).minimize(loss)\n",
    "\n",
    "    # Saver\n",
    "    # saver = tf.train.Saver()\n",
    "\n",
    "    # Collect Info.\n",
    "    losses = []\n",
    "    trainAccuracies = []\n",
    "    devAccuracies = []\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # Variable Initialization.\n",
    "        # if saveIn:\n",
    "        #     saver.restore(sess, saveIn)\n",
    "        # else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch in range(config[\"numEpochs\"]):\n",
    "            # Training.\n",
    "            epochLoss = 0\n",
    "            epochAccuracy = 0\n",
    "            for batchNum, batches in enumerate(get_minibatches(trainData, config[\"batchSize\"])):\n",
    "                feedDict = {\n",
    "                    comments: batches[0],\n",
    "                    masks: batches[1],\n",
    "                    labels: batches[5],\n",
    "                    learningRate: config[\"learningRates\"][epoch],\n",
    "                    dropoutKeepProb: config[\"dropoutKeepProb\"]\n",
    "                }\n",
    "                if config[\"addCommentp\"]:\n",
    "                    feedDict[commentps] = batches[2]\n",
    "                    feedDict[maskps] = batches[3]\n",
    "                if config[\"addCommentf\"]:\n",
    "                    feedDict[commentfs] = batches[4]\n",
    "\n",
    "                batchSize = len(batches[0])\n",
    "                batchAccuracy, batchLoss, _ = sess.run([accuracy, loss, optimizer], feedDict)\n",
    "                epochLoss += batchLoss * batchSize\n",
    "                epochAccuracy += batchAccuracy * batchSize\n",
    "                if (batchNum + 1) % 100 == 0:\n",
    "                    print \"Epoch: {}, Batch: {}\".format(epoch + 1, batchNum + 1)\n",
    "            losses.append(epochLoss / float(config[\"numTrain\"]))\n",
    "            trainAccuracies.append(epochAccuracy / float(config[\"numTrain\"]))\n",
    "            print \"Epoch: {}, Loss: {}, Accuracy: {}\".format(epoch + 1, losses[-1], trainAccuracies[-1])\n",
    "\n",
    "            # Dev.\n",
    "            epochAccuracy = 0\n",
    "            for batchNum, batches in enumerate(get_minibatches(devData, config[\"batchSize\"])):\n",
    "                feedDict = {\n",
    "                    comments: batches[0],\n",
    "                    masks: batches[1],\n",
    "                    labels: batches[5],\n",
    "                    learningRate: config[\"learningRates\"][epoch],\n",
    "                    dropoutKeepProb: 1.0\n",
    "                }\n",
    "                if config[\"addCommentp\"]:\n",
    "                    feedDict[commentps] = batches[2]\n",
    "                    feedDict[maskps] = batches[3]\n",
    "                if config[\"addCommentf\"]:\n",
    "                    feedDict[commentfs] = batches[4]\n",
    "\n",
    "                batchSize = len(batches[0])\n",
    "                epochAccuracy += sess.run(accuracy, feedDict) * batchSize\n",
    "            devAccuracies.append(epochAccuracy / float(config[\"numDev\"]))\n",
    "            print \"Dev Accuracy: {}\".format(devAccuracies[-1])\n",
    "        print('Confusion Matrix: \\n\\n', tf.Tensor.eval(confusion,feed_dict=None, session=None))\n",
    "\n",
    "            # savePath = saver.save(sess, saveOut)\n",
    "            # print \"Model saved at {}\".format(savePath)\n",
    "\n",
    "    # Print out summary.\n",
    "    bestDevAccuracy = 0\n",
    "    bestIndex = 0\n",
    "    for i, accuracy in enumerate(devAccuracies):\n",
    "        if accuracy > bestDevAccuracy:\n",
    "            bestDevAccuracy = accuracy\n",
    "            bestIndex = i\n",
    "            bestConfusion = confusion\n",
    "\n",
    "    print \"Best Dev of {} at epoch {}, train acc: {}, train loss: {}\".format(\n",
    "        bestDevAccuracy,\n",
    "        bestIndex + 1,\n",
    "        trainAccuracies[bestIndex],\n",
    "        losses[bestIndex])\n",
    "\n",
    "    # Return series.\n",
    "    return losses, trainAccuracies, devAccuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# numLines = float('inf')\n",
    "\n",
    "# vocab, frequencies = loadWordVectors(\"glove.6B.300d.txt\")\n",
    "\n",
    "# print \"Processing Training Data\"\n",
    "# trainComments = processComments(\n",
    "#     \"Reddit2ndTrainTime\",\n",
    "#     numLines,\n",
    "#     vocab,\n",
    "#     frequencies)\n",
    "\n",
    "# print \"Processing Dev Data\"\n",
    "# devComments = processComments(\n",
    "#     \"Reddit2ndDevTime\",\n",
    "#     numLines,\n",
    "#     vocab,\n",
    "#     frequencies)\n",
    "\n",
    "# print \"Cleaning frequencies\"\n",
    "# vocab, embed = cleanFrequencies(vocab, frequencies)\n",
    "# assert len(vocab) == len(embed)\n",
    "# print \"Vocab size: {}\".format(len(vocab))\n",
    "\n",
    "# print \"Outputting train comments\"\n",
    "# outputComments(trainComments, \"data/ProcessedTrain\", vocab)\n",
    "\n",
    "# print \"Outputting dev comments\"\n",
    "# outputComments(devComments, \"data/ProcessedDev\", vocab)\n",
    "\n",
    "# print \"Outputting embeddings\"\n",
    "# np.savetxt(\"data/embed.txt\", embed)\n",
    "\n",
    "# print \"Outputting vocab\"\n",
    "# outputVocab(vocab, \"data/vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config\n",
      "Loading embeddings\n"
     ]
    }
   ],
   "source": [
    "print \"Loading config\"\n",
    "config = getConfig()\n",
    "\n",
    "print \"Loading embeddings\"\n",
    "embed = np.loadtxt(\"data/embed.txt\", dtype=np.float32)\n",
    "print embed.shape\n",
    "\n",
    "print \"Loading Training Data\"\n",
    "trainData = loadComments(\"data/ProcessedTrain\", config[\"numTrain\"], config)\n",
    "\n",
    "print \"Loading Dev Data\"\n",
    "devData = loadComments(\"data/ProcessedDev\", config[\"numDev\"], config)\n",
    "\n",
    "# Additional configs\n",
    "config[\"vocabSize\"] = len(embed)\n",
    "config[\"embedDim\"] = len(embed[0])\n",
    "config[\"numCommentfs\"] = len(trainData[4][0])\n",
    "config[\"numLSTMOutputs\"] = config[\"lstmUnits\"] + config[\"numCommentfs\"]\n",
    "if config[\"addCommentp\"]:\n",
    "    config[\"numLSTMOutputs\"] += config[\"lstmUnits\"]\n",
    "printConfig(config)\n",
    "# config[\"addTime\"] = True\n",
    "# config[\"addCommentp\"] = True\n",
    "\n",
    "print \"Training\"\n",
    "losses, trainAccuracies, devAccuracies = train(\n",
    "    embed, \n",
    "    trainData, \n",
    "    devData, \n",
    "    config,\n",
    "    trainableE=False\n",
    "    )\n",
    "\n",
    "print \"Plotting\"\n",
    "plot(losses, trainAccuracies, devAccuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 lines.\n",
      "Processed 20 lines.\n",
      "Processed 30 lines.\n",
      "Processed 40 lines.\n",
      "Processed 50 lines.\n",
      "Processed 60 lines.\n",
      "Processed 70 lines.\n",
      "Processed 80 lines.\n",
      "Processed 90 lines.\n",
      "Processed 100 lines.\n",
      "Processed 110 lines.\n",
      "Processed 120 lines.\n",
      "Processed 130 lines.\n",
      "Processed 140 lines.\n",
      "Processed 150 lines.\n",
      "Processed 160 lines.\n",
      "Processed 170 lines.\n",
      "Processed 180 lines.\n",
      "Processed 190 lines.\n",
      "Processed 200 lines.\n",
      "Processed 210 lines.\n",
      "Processed 220 lines.\n",
      "Processed 230 lines.\n",
      "Processed 240 lines.\n",
      "Processed 250 lines.\n",
      "Processed 260 lines.\n",
      "Processed 270 lines.\n",
      "Processed 280 lines.\n",
      "Processed 290 lines.\n",
      "Processed 300 lines.\n",
      "Processed 310 lines.\n",
      "Processed 320 lines.\n",
      "Processed 330 lines.\n",
      "Processed 340 lines.\n",
      "Processed 350 lines.\n",
      "Processed 360 lines.\n",
      "Processed 370 lines.\n",
      "Processed 380 lines.\n",
      "Processed 390 lines.\n",
      "Processed 400 lines.\n",
      "Processed 410 lines.\n",
      "Processed 420 lines.\n",
      "Processed 430 lines.\n",
      "Processed 440 lines.\n",
      "Processed 450 lines.\n",
      "Processed 460 lines.\n",
      "Processed 470 lines.\n",
      "Processed 480 lines.\n",
      "Processed 490 lines.\n",
      "Processed 500 lines.\n",
      "Processed 510 lines.\n",
      "Processed 520 lines.\n",
      "Processed 530 lines.\n",
      "Processed 540 lines.\n",
      "Processed 550 lines.\n",
      "Processed 560 lines.\n",
      "Processed 570 lines.\n",
      "Processed 580 lines.\n",
      "Processed 590 lines.\n",
      "Processed 600 lines.\n",
      "Processed 610 lines.\n",
      "Processed 620 lines.\n",
      "Processed 630 lines.\n",
      "Processed 640 lines.\n",
      "Processed 650 lines.\n",
      "Processed 660 lines.\n",
      "Processed 670 lines.\n",
      "Processed 680 lines.\n",
      "Processed 690 lines.\n",
      "Processed 700 lines.\n",
      "Processed 710 lines.\n",
      "Processed 720 lines.\n",
      "Processed 730 lines.\n",
      "Processed 740 lines.\n",
      "Processed 750 lines.\n",
      "Processed 760 lines.\n",
      "Processed 770 lines.\n",
      "Processed 780 lines.\n",
      "Processed 790 lines.\n",
      "Processed 800 lines.\n",
      "Processed 810 lines.\n",
      "Processed 820 lines.\n",
      "Processed 830 lines.\n",
      "Processed 840 lines.\n",
      "Processed 850 lines.\n",
      "Processed 860 lines.\n",
      "Processed 870 lines.\n",
      "Processed 880 lines.\n",
      "Processed 890 lines.\n",
      "Processed 900 lines.\n",
      "Processed 910 lines.\n",
      "Processed 920 lines.\n",
      "Processed 930 lines.\n",
      "Processed 940 lines.\n",
      "Processed 950 lines.\n",
      "Processed 960 lines.\n",
      "Processed 970 lines.\n",
      "Processed 980 lines.\n",
      "Processed 990 lines.\n",
      "Processed 1000 lines.\n",
      "Processed 1010 lines.\n",
      "Processed 1020 lines.\n",
      "Processed 1030 lines.\n",
      "Processed 1040 lines.\n",
      "Processed 1050 lines.\n",
      "Processed 1060 lines.\n",
      "Processed 1070 lines.\n",
      "Processed 1080 lines.\n",
      "Processed 1090 lines.\n",
      "Processed 1100 lines.\n",
      "Processed 1110 lines.\n",
      "Processed 1120 lines.\n",
      "Processed 1130 lines.\n",
      "Processed 1140 lines.\n",
      "Processed 1150 lines.\n",
      "Processed 1160 lines.\n",
      "Processed 1170 lines.\n",
      "Processed 1180 lines.\n",
      "Processed 1190 lines.\n",
      "Processed 1200 lines.\n",
      "Processed 1210 lines.\n",
      "Processed 1220 lines.\n",
      "Processed 1230 lines.\n",
      "Processed 1240 lines.\n",
      "Processed 1250 lines.\n",
      "Processed 1260 lines.\n",
      "Processed 1270 lines.\n",
      "Processed 1280 lines.\n",
      "Processed 1290 lines.\n",
      "Processed 1300 lines.\n",
      "Processed 1310 lines.\n",
      "Processed 1320 lines.\n",
      "Processed 1330 lines.\n",
      "Processed 1340 lines.\n",
      "Processed 1350 lines.\n",
      "Processed 1360 lines.\n",
      "Processed 1370 lines.\n",
      "Processed 1380 lines.\n",
      "Processed 1390 lines.\n",
      "Processed 1400 lines.\n",
      "Processed 1410 lines.\n",
      "Processed 1420 lines.\n",
      "Processed 1430 lines.\n",
      "Processed 1440 lines.\n",
      "Processed 1450 lines.\n",
      "Processed 1460 lines.\n",
      "Processed 1470 lines.\n",
      "Processed 1480 lines.\n",
      "Processed 1490 lines.\n",
      "Processed 1500 lines.\n",
      "Processed 1510 lines.\n",
      "Processed 1520 lines.\n",
      "Processed 1530 lines.\n",
      "Processed 1540 lines.\n",
      "Processed 1550 lines.\n",
      "Processed 1560 lines.\n",
      "Processed 1570 lines.\n",
      "Processed 1580 lines.\n",
      "Processed 1590 lines.\n",
      "Processed 1600 lines.\n",
      "Processed 1610 lines.\n",
      "Processed 1620 lines.\n",
      "Processed 1630 lines.\n",
      "Processed 1640 lines.\n",
      "Processed 1650 lines.\n",
      "Processed 1660 lines.\n",
      "Processed 1670 lines.\n",
      "Processed 1680 lines.\n",
      "Processed 1690 lines.\n",
      "Processed 1700 lines.\n",
      "Processed 1710 lines.\n",
      "Processed 1720 lines.\n",
      "Processed 1730 lines.\n",
      "Processed 1740 lines.\n",
      "Processed 1750 lines.\n",
      "Processed 1760 lines.\n",
      "Processed 1770 lines.\n",
      "Processed 1780 lines.\n",
      "Processed 1790 lines.\n",
      "Processed 1800 lines.\n",
      "Processed 1810 lines.\n",
      "Processed 1820 lines.\n",
      "Processed 1830 lines.\n",
      "Processed 1840 lines.\n",
      "Processed 1850 lines.\n",
      "Processed 1860 lines.\n",
      "Processed 1870 lines.\n",
      "Processed 1880 lines.\n",
      "Processed 1890 lines.\n",
      "Processed 1900 lines.\n",
      "Processed 1910 lines.\n",
      "Processed 1920 lines.\n",
      "Processed 1930 lines.\n",
      "Processed 1940 lines.\n",
      "Processed 1950 lines.\n",
      "Processed 1960 lines.\n",
      "Processed 1970 lines.\n",
      "Processed 1980 lines.\n",
      "Processed 1990 lines.\n",
      "Processed 2000 lines.\n",
      "Processed 2010 lines.\n",
      "Processed 2020 lines.\n",
      "Processed 2030 lines.\n",
      "Processed 2040 lines.\n",
      "Processed 2050 lines.\n",
      "Processed 2060 lines.\n",
      "Processed 2070 lines.\n",
      "Processed 2080 lines.\n",
      "Processed 2090 lines.\n",
      "Processed 2100 lines.\n",
      "Processed 2110 lines.\n",
      "Processed 2120 lines.\n",
      "Processed 2130 lines.\n",
      "Processed 2140 lines.\n",
      "Processed 2150 lines.\n",
      "Processed 2160 lines.\n",
      "Processed 2170 lines.\n",
      "Processed 2180 lines.\n",
      "Processed 2190 lines.\n",
      "Processed 2200 lines.\n",
      "Processed 2210 lines.\n",
      "Processed 2220 lines.\n",
      "Processed 2230 lines.\n",
      "Processed 2240 lines.\n",
      "Processed 2250 lines.\n",
      "Processed 2260 lines.\n",
      "Processed 2270 lines.\n",
      "Processed 2280 lines.\n",
      "Processed 2290 lines.\n",
      "Processed 2300 lines.\n",
      "Processed 2310 lines.\n",
      "Processed 2320 lines.\n",
      "Processed 2330 lines.\n",
      "Processed 2340 lines.\n",
      "Processed 2350 lines.\n",
      "Processed 2360 lines.\n",
      "Processed 2370 lines.\n",
      "Processed 2380 lines.\n",
      "Processed 2390 lines.\n",
      "Processed 2400 lines.\n",
      "Processed 2410 lines.\n",
      "Processed 2420 lines.\n",
      "Processed 2430 lines.\n",
      "Processed 2440 lines.\n",
      "Processed 2450 lines.\n",
      "Processed 2460 lines.\n",
      "Processed 2470 lines.\n",
      "Processed 2480 lines.\n",
      "Processed 2490 lines.\n",
      "Processed 2500 lines.\n",
      "Processed 2510 lines.\n",
      "Processed 2520 lines.\n",
      "Processed 2530 lines.\n",
      "Processed 2540 lines.\n",
      "Processed 2550 lines.\n",
      "Processed 2560 lines.\n",
      "Processed 2570 lines.\n",
      "Processed 2580 lines.\n",
      "Processed 2590 lines.\n",
      "Processed 2600 lines.\n",
      "Processed 2610 lines.\n",
      "Processed 2620 lines.\n",
      "Processed 2630 lines.\n",
      "Processed 2640 lines.\n",
      "Processed 2650 lines.\n",
      "Processed 2660 lines.\n",
      "Processed 2670 lines.\n",
      "Processed 2680 lines.\n",
      "Processed 2690 lines.\n",
      "Processed 2700 lines.\n",
      "Processed 2710 lines.\n",
      "Processed 2720 lines.\n",
      "Processed 2730 lines.\n",
      "Processed 2740 lines.\n",
      "Processed 2750 lines.\n",
      "Processed 2760 lines.\n",
      "Processed 2770 lines.\n",
      "Processed 2780 lines.\n",
      "Processed 2790 lines.\n",
      "Processed 2800 lines.\n",
      "Processed 2810 lines.\n",
      "Processed 2820 lines.\n",
      "Processed 2830 lines.\n",
      "Processed 2840 lines.\n",
      "Processed 2850 lines.\n",
      "Processed 2860 lines.\n",
      "Processed 2870 lines.\n",
      "Processed 2880 lines.\n",
      "Processed 2890 lines.\n",
      "Processed 2900 lines.\n",
      "Processed 2910 lines.\n",
      "Processed 2920 lines.\n",
      "Processed 2930 lines.\n",
      "Processed 2940 lines.\n",
      "Processed 2950 lines.\n",
      "Processed 2960 lines.\n",
      "Processed 2970 lines.\n",
      "Processed 2980 lines.\n",
      "Processed 2990 lines.\n",
      "Processed 3000 lines.\n",
      "Processed 3010 lines.\n",
      "Processed 3020 lines.\n",
      "Processed 3030 lines.\n",
      "Processed 3040 lines.\n",
      "Processed 3050 lines.\n",
      "Processed 3060 lines.\n",
      "Processed 3070 lines.\n",
      "Processed 3080 lines.\n",
      "Processed 3090 lines.\n",
      "Processed 3100 lines.\n",
      "Processed 3110 lines.\n",
      "Processed 3120 lines.\n",
      "Processed 3130 lines.\n",
      "Processed 3140 lines.\n",
      "Processed 3150 lines.\n",
      "Processed 3160 lines.\n",
      "Processed 3170 lines.\n",
      "Processed 3180 lines.\n",
      "Processed 3190 lines.\n",
      "Processed 3200 lines.\n",
      "Processed 3210 lines.\n",
      "Processed 3220 lines.\n",
      "Processed 3230 lines.\n",
      "Processed 3240 lines.\n",
      "Processed 3250 lines.\n",
      "Processed 3260 lines.\n",
      "Processed 3270 lines.\n",
      "Processed 3280 lines.\n",
      "Processed 3290 lines.\n",
      "Processed 3300 lines.\n",
      "Processed 3310 lines.\n",
      "Processed 3320 lines.\n",
      "Processed 3330 lines.\n",
      "Processed 3340 lines.\n",
      "Processed 3350 lines.\n",
      "Processed 3360 lines.\n",
      "Processed 3370 lines.\n",
      "Processed 3380 lines.\n",
      "Processed 3390 lines.\n",
      "Processed 3400 lines.\n",
      "Processed 3410 lines.\n",
      "Processed 3420 lines.\n",
      "Processed 3430 lines.\n",
      "Processed 3440 lines.\n",
      "Processed 3450 lines.\n",
      "Processed 3460 lines.\n",
      "Processed 3470 lines.\n",
      "Processed 3480 lines.\n",
      "Processed 3490 lines.\n",
      "Processed 3500 lines.\n",
      "Processed 3510 lines.\n",
      "Processed 3520 lines.\n",
      "Processed 3530 lines.\n",
      "Processed 3540 lines.\n",
      "Processed 3550 lines.\n",
      "Processed 3560 lines.\n",
      "Processed 3570 lines.\n",
      "Processed 3580 lines.\n",
      "Processed 3590 lines.\n",
      "Processed 3600 lines.\n",
      "Processed 3610 lines.\n",
      "Processed 3620 lines.\n",
      "Processed 3630 lines.\n",
      "Processed 3640 lines.\n",
      "Processed 3650 lines.\n",
      "Processed 3660 lines.\n",
      "Processed 3670 lines.\n",
      "Processed 3680 lines.\n",
      "Processed 3690 lines.\n",
      "Processed 3700 lines.\n",
      "Processed 3710 lines.\n",
      "Processed 3720 lines.\n",
      "Processed 3730 lines.\n",
      "Processed 3740 lines.\n",
      "Processed 3750 lines.\n",
      "Processed 3760 lines.\n",
      "Processed 3770 lines.\n",
      "Processed 3780 lines.\n",
      "Processed 3790 lines.\n",
      "Processed 3800 lines.\n",
      "Processed 3810 lines.\n",
      "Processed 3820 lines.\n",
      "Processed 3830 lines.\n",
      "Processed 3840 lines.\n",
      "Processed 3850 lines.\n",
      "Processed 3860 lines.\n",
      "Processed 3870 lines.\n",
      "Processed 3880 lines.\n",
      "Processed 3890 lines.\n",
      "Processed 3900 lines.\n",
      "Processed 3910 lines.\n",
      "Processed 3920 lines.\n",
      "Processed 3930 lines.\n",
      "Processed 3940 lines.\n",
      "Processed 3950 lines.\n",
      "Processed 3960 lines.\n",
      "Processed 3970 lines.\n",
      "Processed 3980 lines.\n",
      "Processed 3990 lines.\n",
      "Processed 4000 lines.\n",
      "Processed 4010 lines.\n",
      "Processed 4020 lines.\n",
      "Processed 4030 lines.\n",
      "Processed 4040 lines.\n",
      "Processed 4050 lines.\n",
      "Processed 4060 lines.\n",
      "Processed 4070 lines.\n",
      "Processed 4080 lines.\n",
      "Processed 4090 lines.\n",
      "Processed 4100 lines.\n",
      "Processed 4110 lines.\n",
      "Processed 4120 lines.\n",
      "Processed 4130 lines.\n",
      "Processed 4140 lines.\n",
      "Processed 4150 lines.\n",
      "Processed 4160 lines.\n",
      "Processed 4170 lines.\n",
      "Processed 4180 lines.\n",
      "Processed 4190 lines.\n",
      "Processed 4200 lines.\n",
      "Processed 4210 lines.\n",
      "Processed 4220 lines.\n",
      "Processed 4230 lines.\n",
      "Processed 4240 lines.\n",
      "Processed 4250 lines.\n",
      "Processed 4260 lines.\n",
      "Processed 4270 lines.\n",
      "Processed 4280 lines.\n",
      "Processed 4290 lines.\n",
      "Processed 4300 lines.\n",
      "Processed 4310 lines.\n",
      "Processed 4320 lines.\n",
      "Processed 4330 lines.\n",
      "Processed 4340 lines.\n",
      "Processed 4350 lines.\n",
      "Processed 4360 lines.\n",
      "Processed 4370 lines.\n",
      "Processed 4380 lines.\n",
      "Processed 4390 lines.\n",
      "Processed 4400 lines.\n",
      "Processed 4410 lines.\n",
      "Processed 4420 lines.\n",
      "Processed 4430 lines.\n",
      "Processed 4440 lines.\n",
      "Processed 4450 lines.\n",
      "Processed 4460 lines.\n",
      "Processed 4470 lines.\n",
      "Processed 4480 lines.\n",
      "Processed 4490 lines.\n",
      "Processed 4500 lines.\n",
      "Processed 4510 lines.\n",
      "Processed 4520 lines.\n",
      "Processed 4530 lines.\n",
      "Processed 4540 lines.\n",
      "Processed 4550 lines.\n",
      "Processed 4560 lines.\n",
      "Processed 4570 lines.\n",
      "Processed 4580 lines.\n",
      "Processed 4590 lines.\n",
      "Processed 4600 lines.\n",
      "Processed 4610 lines.\n",
      "Processed 4620 lines.\n",
      "Processed 4630 lines.\n",
      "Processed 4640 lines.\n",
      "Processed 4650 lines.\n",
      "Processed 4660 lines.\n",
      "Processed 4670 lines.\n",
      "Processed 4680 lines.\n",
      "Processed 4690 lines.\n",
      "Processed 4700 lines.\n",
      "Processed 4710 lines.\n",
      "Processed 4720 lines.\n",
      "Processed 4730 lines.\n",
      "Processed 4740 lines.\n",
      "Processed 4750 lines.\n",
      "Processed 4760 lines.\n",
      "Processed 4770 lines.\n",
      "Processed 4780 lines.\n",
      "Processed 4790 lines.\n",
      "Processed 4800 lines.\n",
      "Processed 4810 lines.\n",
      "Processed 4820 lines.\n",
      "Processed 4830 lines.\n",
      "Processed 4840 lines.\n",
      "Processed 4850 lines.\n",
      "Processed 4860 lines.\n",
      "Processed 4870 lines.\n",
      "Processed 4880 lines.\n",
      "Processed 4890 lines.\n",
      "Processed 4900 lines.\n",
      "Processed 4910 lines.\n",
      "Processed 4920 lines.\n",
      "Processed 4930 lines.\n",
      "Processed 4940 lines.\n",
      "Processed 4950 lines.\n",
      "Processed 4960 lines.\n",
      "Processed 4970 lines.\n",
      "Processed 4980 lines.\n",
      "Processed 4990 lines.\n",
      "Processed 5000 lines.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "corrects = []\n",
    "incorrects = []\n",
    "with open(\"lstmSummaryPred.json\", \"r\") as inFile:\n",
    "    for i, line in enumerate(inFile, 1):\n",
    "        comment = json.loads(line)\n",
    "        correct = 0\n",
    "        if (comment[\"prediction\"] == 1 and comment[\"num_child_comments\"] > 0) or (comment[\"prediction\"] == 0 and comment[\"num_child_comments\"] == 0):\n",
    "            correct = 1\n",
    "        if correct == 1:\n",
    "            corrects.append(comment)\n",
    "        else:\n",
    "            incorrects.append(comment)\n",
    "        if i % 10 == 0: print(\"Processed {} lines.\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def check_correlation(corrects, incorrects, feature):\n",
    "    X1 = []\n",
    "    y1 = []\n",
    "    X0 = []\n",
    "    y0 = []\n",
    "    if feature == \"length\":\n",
    "        for x in corrects:\n",
    "            y1.append(1)\n",
    "            X1.append(len(x[\"body\"].split()))\n",
    "        for x in incorrects:\n",
    "            y0.append(0)\n",
    "            X0.append(len(x[\"body\"].split()))\n",
    "    elif feature == \"positivechildren\":\n",
    "        for x in corrects:\n",
    "            if x['prediction'] > 0:\n",
    "                y1.append(1)\n",
    "                X1.append(x[\"num_child_comments\"])\n",
    "        for x in incorrects:\n",
    "            if x['prediction'] > 0:\n",
    "                y0.append(0)\n",
    "                X0.append(x[\"num_child_comments\"])\n",
    "    elif feature == \"negativechildren\":\n",
    "        for x in corrects:\n",
    "            if x['prediction'] < 1:\n",
    "                y1.append(1)\n",
    "                X1.append(x[\"num_child_comments\"])\n",
    "        for x in incorrects:\n",
    "            if x['prediction'] < 1:\n",
    "                y0.append(0)\n",
    "                X0.append(x[\"num_child_comments\"])\n",
    "    else:\n",
    "        for x in corrects:\n",
    "            y1.append(1)\n",
    "            X1.append(x[feature])\n",
    "        for x in incorrects:\n",
    "            y0.append(0)\n",
    "            X0.append(x[feature])\n",
    "    X = np.concatenate((X1,X0))\n",
    "    y = np.concatenate((y1,y0))\n",
    "    p = pearsonr(list(X), list(y))\n",
    "    print(\"Pearson Correlation Coefficient: {}\".format(p))\n",
    "        \n",
    "    print(\"Correct Mean: {}, Incorrect Mean: {}\".format(np.mean(X1), np.mean(X0)))\n",
    "    print(\"Correct Median: {}, Incorrect Median: {}\".format(np.median(X1), np.median(X0)))\n",
    "    print(\"Correct Std: {}, Incorrect Std: {}\".format(np.std(X1), np.std(X0)))\n",
    "\n",
    "    xlabel = feature\n",
    "    \n",
    "#     plt.hist(X1)\n",
    "#     plt.xlabel(xlabel)\n",
    "#     plt.ylabel(\"Occurrence\")\n",
    "#     plt.title(\"{} of Corrects\".format(feature))\n",
    "#     plt.grid(True)\n",
    "#     plt.show()\n",
    "    \n",
    "#     plt.hist(X0)\n",
    "#     plt.xlabel(xlabel)\n",
    "#     plt.ylabel(\"Occurrence\")\n",
    "#     plt.title(\"{} of Incorrects\".format(feature))\n",
    "#     plt.grid(True)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlation Coefficient: (0.027090055827073817, 0.055437844487939175)\n",
      "Correct Mean: 43.3674475956, Incorrect Mean: 39.9954441913\n",
      "Correct Median: 25.0, Incorrect Median: 23.0\n",
      "Correct Std: 60.0773340566, Incorrect Std: 58.1139892138\n"
     ]
    }
   ],
   "source": [
    "check_correlation(corrects, incorrects, \"length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlation Coefficient: (0.003991702049542469, 0.7778005010572098)\n",
      "Correct Mean: 2.71085080148, Incorrect Mean: 2.6930523918\n",
      "Correct Median: 2.0, Incorrect Median: 2.0\n",
      "Correct Std: 2.13744564268, Incorrect Std: 2.11157392517\n"
     ]
    }
   ],
   "source": [
    "check_correlation(corrects, incorrects, \"weekday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlation Coefficient: (0.00589288468065817, 0.6769788340939595)\n",
      "Correct Mean: 2.81257706535, Incorrect Mean: 2.78986332574\n",
      "Correct Median: 3.0, Incorrect Median: 3.0\n",
      "Correct Std: 1.83556846993, Incorrect Std: 1.84777455988\n"
     ]
    }
   ],
   "source": [
    "check_correlation(corrects, incorrects, \"time_of_day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlation Coefficient: (0.31739853367414167, 4.9383306196631316e-79)\n",
      "Correct Mean: 5.4781420765, Incorrect Mean: 0.0\n",
      "Correct Median: 3.0, Incorrect Median: 0.0\n",
      "Correct Std: 9.57602954796, Incorrect Std: 0.0\n"
     ]
    }
   ],
   "source": [
    "check_correlation(corrects, incorrects, \"positivechildren\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlation Coefficient: (-0.32617383457390564, 1.7941953668691163e-42)\n",
      "Correct Mean: 0.0, Incorrect Mean: 3.79934747145\n",
      "Correct Median: 0.0, Incorrect Median: 2.0\n",
      "Correct Std: 0.0, Incorrect Std: 8.74641103175\n"
     ]
    }
   ],
   "source": [
    "check_correlation(corrects, incorrects, \"negativechildren\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs224]",
   "language": "python",
   "name": "conda-env-cs224-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
